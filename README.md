# Fine-Tuning NLLB for Low-Resource Odia-German Translation

This repository contains the complete source code, datasets, experimental results, and model artifacts for the thesis titled: **"Enhancing Contextual Understanding in Low-Resource Languages Using Multilingual Transformers"**

The research investigates and compares two primary methodologies for adapting a state-of-the-art multilingual model (`facebook/nllb-200-distilled-600M`) for a specialized, bidirectional Odia-German translation task in the journalistic domain: **Full Fine-Tuning (FFT)** and **Parameter-Efficient Fine-Tuning (PEFT) using LoRA**.

---

## Live Demonstrations

Interactive web applications for both the Fully Fine-Tuned and Adapter-Tuned (LoRA) models are publicly available on Hugging Face Spaces:

* **[Live Demo: Fully Fine-Tuned Model]([https://huggingface.co/spaces/abhinandansamal/full_fine_tuned_model_web_application](https://huggingface.co/spaces/abhinandansamal/Odia-German-Translator))** 
* **[Live Demo: Adapter-Tuned (LoRA) Model]([https://huggingface.co/spaces/abhinandansamal/Adapter_based_fine_tuned_model_web_application](https://huggingface.co/spaces/abhinandansamal/Adapter-based-Odia-German-Translator))**

---

## Repository Structure

The project is organized into a clear, reproducible structure. The main directories are:
