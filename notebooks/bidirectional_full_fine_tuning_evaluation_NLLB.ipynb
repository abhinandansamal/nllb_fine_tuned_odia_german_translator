{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq1_3dtzhzJp"
      },
      "source": [
        "# NLLB Full Fine-Tuning and Evaluation\n",
        "## Objective\n",
        "The objective of this notebook is to execute a core experimental workflow of the thesis: to **fully fine-tune** the state-of-the-art NLLB model on a custom bidirectional corpus and conduct a comprehensive evaluation to measure its performance against a zero-shot baseline. This script serves as a self-contained experiment, generating the quantitative metrics and visualizations necessary to validate the effectiveness of the full fine-tuning adaptation strategy.\n",
        "\n",
        "## Methodology\n",
        "The notebook follows a rigorous, sequential experimental design:\n",
        "\n",
        "1. **Data Preparation:** Loads the structured, bidirectional `.jsonl` corpus, splits it into training (80%), validation (10%), and test (10%) sets, and tokenizes the text for the NLLB model.\n",
        "2. **Baseline Evaluation:** Establishes a performance benchmark by evaluating the original, pre-trained NLLB model on the held-out test set for both `Odia → German` and `German → Odia` directions.\n",
        "3. **Full Fine-Tuning:** Trains the model on the bidirectional corpus, updating all of its 617 million parameters to specialize it for the new task.\n",
        "4. **Final Evaluation:** Evaluates the newly fine-tuned model on the same held-out test set.\n",
        "5. **Results Synthesis:** Consolidates all performance metrics (BLEU, chrF, TER) into a final comparison table and generates visualizations to analyze the results.\n",
        "\n",
        "## Workflow\n",
        "1. Mounts Google Drive for data access.\n",
        "\n",
        "2. Installs all required libraries (`transformers`, `datasets`, etc.).\n",
        "\n",
        "3. Loads and splits the `bidirectional_corpus.jsonl` file.\n",
        "\n",
        "4. Tokenizes the datasets.\n",
        "\n",
        "5. Performs the baseline evaluation.\n",
        "\n",
        "6. Executes the Full Fine-Tuning training run.\n",
        "\n",
        "7. Performs the final evaluation on the fine-tuned model.\n",
        "\n",
        "8. Generates and displays the final summary table and all visualizations comparing the Baseline and the Fully Fine-Tuned model.\n",
        "\n",
        "## Input & Output\n",
        "* **Input:** A single `.jsonl` file (`bidirectional_corpus.jsonl`).\n",
        "* **Output:**\n",
        "  * Saved model artifacts for the Fully Fine-Tuned model in a specified Google Drive directory.\n",
        "  * A printed summary table comparing the performance of the Baseline and the Fully Fine-Tuned model.\n",
        "  * A series of plots visualizing the final performance comparison and training dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C62VFjUhxRY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNKHaBJdh6D1"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets sacrebleu torch accelerate pandas bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- All Installed Packages (pip list) ---\")\n",
        "!pip list"
      ],
      "metadata": {
        "id": "5CkNH_l4WrIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh8LHdAUh8HT"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sacrebleu\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, pipeline\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70g9clKJiA65"
      },
      "source": [
        "# Configuration and Data Preparation\n",
        "\n",
        "This code block sets up all parameters, loads your dataset, and splits it into the crucial train, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBizaESah-xo"
      },
      "outputs": [],
      "source": [
        "# --- CONFIGURATION ---\n",
        "\n",
        "# --- File and Model Configuration ---\n",
        "CORPUS_FILE = \"/content/drive/MyDrive/Thesis/data/transformed/bidirectional_corpus_final.jsonl\"\n",
        "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Thesis/eval/nllb-finetuned-odia-german-model-results\"\n",
        "FINAL_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/nllb-odia-german-translator_model_final\"\n",
        "\n",
        "# --- Field Names ---\n",
        "INPUT_FIELD = \"input_text\" # As defined in the JSONL\n",
        "TARGET_FIELD = \"target_text\" # As defined in the JSONL\n",
        "\n",
        "# --- Language Configuration ---\n",
        "ODIA_LANG_CODE = \"ory_Orya\"\n",
        "GERMAN_LANG_CODE = \"deu_Latn\"\n",
        "\n",
        "# --- Split Configuration ---\n",
        "TEST_SET_SIZE = 0.10   # 10% of the data will be for the final, unseen test set\n",
        "VALIDATION_SET_SIZE = 0.10 # 10% of the remaining data for validation\n",
        "\n",
        "# --- Task Prefixes ---\n",
        "PREFIX_ORI_TO_DEU = \"translate Odia to German: \"\n",
        "PREFIX_DEU_TO_ORI = \"translate German to Odia: \""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD AND SPLIT THE DATASET ---\n",
        "\n",
        "print(\"Loading the rich bidirectional dataset...\")\n",
        "\n",
        "# --- Manually load the JSONL file into a list of dictionaries ---\n",
        "data_list = []\n",
        "with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data_list.append(json.loads(line))\n",
        "\n",
        "# --- Create a pandas DataFrame ---\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# --- Convert the DataFrame into a Hugging Face Dataset object ---\n",
        "# This bypasses the caching issue.\n",
        "full_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# --- Shuffle the dataset before splitting ---\n",
        "full_dataset = full_dataset.shuffle(seed=42)\n",
        "\n",
        "# --- Create the Test Split ---\n",
        "# This first split separates the final, unseen test set.\n",
        "train_valid_split = full_dataset.train_test_split(test_size=TEST_SET_SIZE, seed=42)\n",
        "test_dataset = train_valid_split['test']\n",
        "\n",
        "# --- Create the Train and Validation Splits ---\n",
        "# The remaining data is split again to create the training and validation sets.\n",
        "train_val_split = train_valid_split['train'].train_test_split(test_size=VALIDATION_SET_SIZE / (1 - TEST_SET_SIZE), seed=42)\n",
        "train_dataset = train_val_split['train']\n",
        "validation_dataset = train_val_split['test']\n",
        "\n",
        "# Combine into a single DatasetDict for convenience\n",
        "split_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': validation_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(\"\\nDataset successfully split:\")\n",
        "print(f\"Training set size: {len(split_datasets['train'])}\")\n",
        "print(f\"Validation set size: {len(split_datasets['validation'])}\")\n",
        "print(f\"Test set size: {len(split_datasets['test'])}\")"
      ],
      "metadata": {
        "id": "m82WUBlu8HQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Corpus Analysis (EDA) ---\n",
        "\n",
        "print(\"\\n--- Starting Corpus Analysis (EDA) ---\")\n",
        "\n",
        "# --- Step A: Reconstruct the Original Parallel Corpus ---\n",
        "# The bidirectional dataset contains two entries for each original sentence.\n",
        "# filter for just one direction to get a clean, non-duplicated list of the original pairs.\n",
        "print(\"Reconstructing original sentence pairs from bidirectional data...\")\n",
        "\n",
        "# Use the full dataset before it was split to get stats on the entire corpus.\n",
        "odia_source_sentences = [\n",
        "    ex[INPUT_FIELD].replace(PREFIX_ORI_TO_DEU, \"\")\n",
        "    for ex in full_dataset\n",
        "    if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)\n",
        "]\n",
        "german_target_sentences = [\n",
        "    ex[TARGET_FIELD]\n",
        "    for ex in full_dataset\n",
        "    if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)\n",
        "]\n",
        "\n",
        "print(f\"Successfully extracted {len(odia_source_sentences)} unique parallel pairs.\")\n",
        "\n",
        "\n",
        "# --- Step B: Calculate Sentence Lengths ---\n",
        "# Calculate the number of words by splitting on spaces.\n",
        "odia_lengths = [len(s.split()) for s in odia_source_sentences]\n",
        "german_lengths = [len(s.split()) for s in german_target_sentences]"
      ],
      "metadata": {
        "id": "CaEunqqK8VOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step C: Calculate and Display Descriptive Statistics ---\n",
        "stats_data = {\n",
        "    'Language': ['Odia', 'German'],\n",
        "    'Mean Length': [np.mean(odia_lengths), np.mean(german_lengths)],\n",
        "    'Median Length': [np.median(odia_lengths), np.median(german_lengths)],\n",
        "    'Std. Deviation': [np.std(odia_lengths), np.std(german_lengths)],\n",
        "    'Max Length': [np.max(odia_lengths), np.max(german_lengths)]\n",
        "}\n",
        "df_stats = pd.DataFrame(stats_data).set_index('Language')\n",
        "\n",
        "print(\"\\n--- Corpus Statistics ---\")\n",
        "# Display the formatted table\n",
        "display(df_stats.style.format(\"{:.2f}\"))"
      ],
      "metadata": {
        "id": "LheNhOFP8qBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step D: Visualize Sentence Length Distribution ---\n",
        "print(\"\\n--- Generating Sentence Length Distribution Plot ---\")\n",
        "\n",
        "# Prepare data for plotting with Seaborn\n",
        "plot_df_odia = pd.DataFrame({'Sentence Length (words)': odia_lengths})\n",
        "plot_df_odia['Language'] = 'Odia'\n",
        "plot_df_german = pd.DataFrame({'Sentence Length (words)': german_lengths})\n",
        "plot_df_german['Language'] = 'German'\n",
        "combined_plot_df = pd.concat([plot_df_odia, plot_df_german])\n",
        "\n",
        "# Create the histogram plot\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "fig.suptitle('Figure 4.1: Distribution of Sentence Lengths in the Corpus', fontsize=18, y=1.03)\n",
        "\n",
        "# Plot for Odia\n",
        "sns.histplot(data=plot_df_odia, x='Sentence Length (words)', ax=axes[0], kde=True, bins=30)\n",
        "axes[0].set_title('Odia Sentence Lengths', fontsize=14)\n",
        "axes[0].set_xlabel('Length (in words)', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Plot for German\n",
        "sns.histplot(data=plot_df_german, x='Sentence Length (words)', ax=axes[1], kde=True, bins=30, color='orange')\n",
        "axes[1].set_title('German Sentence Lengths', fontsize=14)\n",
        "axes[1].set_xlabel('Length (in words)', fontsize=12)\n",
        "axes[1].set_ylabel('') # No need for a second y-axis label\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Corpus Analysis complete.\")"
      ],
      "metadata": {
        "id": "Rm7_ERRE8wVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Vocabulary Analysis ---\n",
        "def calculate_vocab_size(sentences_list, tokenizer):\n",
        "  \"\"\"\n",
        "  Calculates the number of unique subword tokens in a list of sentences using a tokenizer.\n",
        "\n",
        "  This function tokenizes each sentence in the provided list using the specified tokenizer,\n",
        "  collects all subword tokens into a set to ensure uniqueness, and returns the size of the\n",
        "  resulting vocabulary. A progress bar is displayed using `tqdm` to track tokenization for\n",
        "  large datasets.\n",
        "\n",
        "  Args:\n",
        "    sentences_list (list[str]): A list of sentences to be tokenized.\n",
        "    tokenizer (object): A tokenizer object with a `tokenize` method that splits a sentence\n",
        "    into a list of subword tokens (e.g., a Hugging Face tokenizer).\n",
        "\n",
        "  Returns:\n",
        "    int: The number of unique subword tokens in the tokenized sentences.\n",
        "\n",
        "  Note:\n",
        "    - Requires the `tqdm` library for progress bar visualization.\n",
        "    - Uses a set for efficient storage and counting of unique tokens.\n",
        "    - Suitable for preprocessing tasks in NLP, such as building vocabularies for machine translation models.\n",
        "  \"\"\"\n",
        "  # Using a set is a highly efficient way to store and count unique items.\n",
        "  unique_tokens = set()\n",
        "\n",
        "  print(f\"Tokenizing {len(sentences_list)} sentences...\")\n",
        "  # Using tqdm for a progress bar as this can take a moment for large lists\n",
        "  for sentence in tqdm(sentences_list):\n",
        "    # tokenizer.tokenize() splits a sentence into a list of subword strings\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    # .update() adds all items from the list to the set\n",
        "    unique_tokens.update(tokens)\n",
        "\n",
        "  return len(unique_tokens)\n",
        "\n",
        "# --- Calculate for Odia ---\n",
        "print(\"\\nAnalyzing Odia corpus...\")\n",
        "odia_vocab_size = calculate_vocab_size(odia_source_sentences, tokenizer)\n",
        "\n",
        "# --- Calculate for German ---\n",
        "print(\"\\nAnalyzing German corpus...\")\n",
        "german_vocab_size = calculate_vocab_size(german_target_sentences, tokenizer)\n",
        "\n",
        "# --- Display the final results ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"     Corpus Vocabulary Statistics\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Unique Odia Tokens in Corpus:   {odia_vocab_size}\")\n",
        "print(f\"Unique German Tokens in Corpus: {german_vocab_size}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "fTO0u1X1BguZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. PREPARE TOKENIZER AND PREPROCESSING FUNCTION ---\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=ODIA_LANG_CODE, tgt_lang=GERMAN_LANG_CODE)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  \"\"\"\n",
        "  Tokenizes input and target text fields for model training or evaluation.\n",
        "\n",
        "  This function processes a batch of examples from a dataset, tokenizing the `input_text` and\n",
        "  `target_text` fields using a provided tokenizer. It applies truncation to a maximum length of\n",
        "  1200 tokens and generates input IDs for both inputs and labels, with the latter prepared in the\n",
        "  tokenizer's target context. The resulting dictionary includes tokenized inputs and labels suitable\n",
        "  for training or evaluating a translation model.\n",
        "\n",
        "  Args:\n",
        "    examples (dict): A dictionary containing lists of input and target sentences, with keys\n",
        "                     defined by `INPUT_FIELD` and `TARGET_FIELD` (e.g., 'input_text' and 'target_text').\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing tokenized inputs (`input_ids`, etc.) and labels\n",
        "          (`labels` as input IDs from target text tokenization).\n",
        "\n",
        "  Note:\n",
        "    - Assumes global variables `INPUT_FIELD`, `TARGET_FIELD`, and `tokenizer` are defined.\n",
        "    - Requires a tokenizer compatible with Hugging Face's `transformers` library.\n",
        "    - The maximum length of 1200 tokens is hardcoded for truncation.\n",
        "\n",
        "  Example:\n",
        "    >>> from transformers import AutoTokenizer\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "    >>> INPUT_FIELD = \"input_text\"\n",
        "    >>> TARGET_FIELD = \"target_text\"\n",
        "    >>> examples = {\n",
        "    ...     \"input_text\": [\"ନମସ୍କାର\"],\n",
        "    ...     \"target_text\": [\"Hallo\"]\n",
        "    ... }\n",
        "    >>> result = preprocess_function(examples)\n",
        "    >>> print(result.keys())\n",
        "    dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
        "  \"\"\"\n",
        "  # Access the lists of sentences directly from the batch columns\n",
        "  inputs = examples[INPUT_FIELD]\n",
        "  targets = examples[TARGET_FIELD]\n",
        "\n",
        "  model_inputs = tokenizer(inputs, max_length=1200, truncation=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(targets, max_length=1200, truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "\n",
        "print(\"Tokenizing all dataset splits...\")\n",
        "\n",
        "tokenized_datasets = split_datasets.map(preprocess_function, batched=True, batch_size=16, remove_columns=split_datasets['train'].column_names)\n",
        "print(\"Tokenization complete.\")"
      ],
      "metadata": {
        "id": "P-em8BN48LRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekh4V_DBiGen"
      },
      "source": [
        "# Baseline Evaluation\n",
        "\n",
        "This code block evaluates the original NLLB model on the `test` set before any fine-tuning happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B-S_VqbiG1i"
      },
      "outputs": [],
      "source": [
        "# --- EVALUATION LOGIC ---\n",
        "def evaluate_model_on_task(model_to_eval, tokenizer, test_data, task_prefix, src_lang, tgt_lang, batch_size=8):\n",
        "  \"\"\"\n",
        "  Evaluates a translation model on a specific translation direction using a test dataset.\n",
        "\n",
        "  This function filters test data for sentences starting with a specified task prefix, generates\n",
        "  translations using a translation pipeline, and computes BLEU, CHRF++, and TER scores against\n",
        "  reference translations. It processes the data in batches to manage memory and displays progress\n",
        "  using a `tqdm` progress bar. If no data matches the prefix, it returns zero scores.\n",
        "\n",
        "  Args:\n",
        "    model_to_eval (object): The translation model to evaluate (e.g., a Hugging Face model).\n",
        "    tokenizer (object): The tokenizer associated with the model.\n",
        "    test_data (list[dict]): A list of dictionaries containing input and target sentences,\n",
        "                            with keys defined by `INPUT_FIELD` and `TARGET_FIELD`.\n",
        "    task_prefix (str): A prefix to filter test data for a specific translation direction (e.g., '[ORI_TO_DEU]').\n",
        "    src_lang (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang (str): Target language code (e.g., 'deu_Latn').\n",
        "    batch_size (int, optional): Number of sentences to process per batch. Defaults to 8.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing evaluation metrics:\n",
        "          - 'bleu': BLEU score.\n",
        "          - 'chrf': CHRF++ score (word order 2).\n",
        "          - 'ter': TER score.\n",
        "  \"\"\"\n",
        "  # Filter the test set for the specific task using the full prefix\n",
        "  task_inputs = [ex[INPUT_FIELD] for ex in test_data if ex[INPUT_FIELD].startswith(task_prefix)]\n",
        "  task_references = [[ex[TARGET_FIELD]] for ex in test_data if ex[INPUT_FIELD].startswith(task_prefix)]\n",
        "\n",
        "  if not task_inputs:\n",
        "    print(f\"Warning: No test data found for prefix '{task_prefix}'\")\n",
        "    return {\"bleu\": 0, \"chrf\": 0, \"ter\": 0}\n",
        "\n",
        "  device = 0 if torch.cuda.is_available() else -1\n",
        "  translator = pipeline(\"translation\", model=model_to_eval, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "  decoded_preds = []\n",
        "  print(f\"Generating translations for {len(task_inputs)} sentences in batches of {batch_size}...\")\n",
        "  for i in tqdm(range(0, len(task_inputs), batch_size)):\n",
        "    batch = task_inputs[i : i + batch_size]\n",
        "    predictions = translator(batch, max_length=1200)\n",
        "    decoded_preds.extend([pred['translation_text'] for pred in predictions])\n",
        "\n",
        "  bleu = sacrebleu.BLEU().corpus_score(decoded_preds, task_references).score\n",
        "  chrf = sacrebleu.CHRF(word_order=2).corpus_score(decoded_preds, task_references).score\n",
        "  ter = sacrebleu.TER().corpus_score(decoded_preds, task_references).score\n",
        "\n",
        "  return {\"bleu\": bleu, \"chrf\": chrf, \"ter\": ter}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3dQWnnNkc-L"
      },
      "outputs": [],
      "source": [
        "# --- BASELINE EVALUATION ---\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "print(\"\\n--- Evaluating Baseline Model (Before Fine-Tuning) ---\")\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Evaluating Odia -> German baseline...\")\n",
        "baseline_ori_deu_metrics = evaluate_model_on_task(base_model, tokenizer, split_datasets['test'], PREFIX_ORI_TO_DEU, ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "\n",
        "print(\"Evaluating German -> Odia baseline...\")\n",
        "baseline_deu_ori_metrics = evaluate_model_on_task(base_model, tokenizer, split_datasets['test'], PREFIX_DEU_TO_ORI, GERMAN_LANG_CODE, ODIA_LANG_CODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S33DPIZrWmQ"
      },
      "outputs": [],
      "source": [
        "# Clear memory before training\n",
        "del base_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZSURj-hiJXP"
      },
      "source": [
        "# Full Fine-Tuning\n",
        "\n",
        "This code block sets up and runs the actual fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVk4_s7WiLr6"
      },
      "outputs": [],
      "source": [
        "# --- FULL FINE-TUNING THE MODEL ---\n",
        "print(\"\\n--- Starting Fine-Tuning Process ---\")\n",
        "# Reload the model without quantization for training\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def compute_metrics_for_trainer(eval_preds):\n",
        "  \"\"\"\n",
        "  Computes BLEU score for model evaluation during training.\n",
        "\n",
        "  This function processes model predictions and labels from a trainer's evaluation step,\n",
        "  decodes them using a tokenizer, and calculates the BLEU score using the `sacrebleu` library.\n",
        "  It handles padding tokens and special tokens to ensure accurate metric computation.\n",
        "\n",
        "  Args:\n",
        "    eval_preds (tuple): A tuple containing predictions and labels from the trainer.\n",
        "                        Predictions are token IDs, and labels are token IDs with -100 for ignored positions.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing the BLEU score with key 'bleu'.\n",
        "\n",
        "  Note:\n",
        "    - Assumes a global `tokenizer` variable with a `batch_decode` method and `pad_token_id`.\n",
        "    - Requires `sacrebleu` and `numpy` libraries.\n",
        "    - Designed for use with Hugging Face's `transformers` Trainer API.\n",
        "\n",
        "  Example:\n",
        "    >>> from transformers import AutoTokenizer\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "    >>> eval_preds = (\n",
        "    ...     tokenizer([\"Hallo\"], return_tensors=\"pt\")[\"input_ids\"].numpy(),\n",
        "    ...     tokenizer([\"Hallo\"], return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
        "    ... )\n",
        "    >>> result = compute_metrics_for_trainer(eval_preds)\n",
        "    >>> print(result)\n",
        "    {'bleu': 100.0}\n",
        "  \"\"\"\n",
        "  preds, labels = eval_preds\n",
        "  if isinstance(preds, tuple): preds = preds[0]\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  decoded_labels = [[label] for label in decoded_labels]\n",
        "  result = sacrebleu.BLEU().corpus_score(decoded_preds, decoded_labels)\n",
        "  return {\"bleu\": result.score}\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    gradient_checkpointing=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True,\n",
        "    optim=\"adafactor\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics_for_trainer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct12yfN1iOV9"
      },
      "source": [
        "# Final Evaluation and Comparison\n",
        "\n",
        "This code block evaluates your newly fine-tuned model on the same test set and presents a clear comparison with the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ZqWqS_iQx-"
      },
      "outputs": [],
      "source": [
        "# --- FINAL EVALUATION ---\n",
        "print(\"\\n--- Evaluating Fine-Tuned Model ---\")\n",
        "# Load the fine-tuned model in 8-bit mode for memory-efficient evaluation\n",
        "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(trainer.state.best_model_checkpoint, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "print(\"Evaluating Odia -> German fine-tuned...\")\n",
        "finetuned_ori_deu_metrics = evaluate_model_on_task(finetuned_model, tokenizer, split_datasets['test'], PREFIX_ORI_TO_DEU, ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "\n",
        "print(\"Evaluating German -> Odia fine-tuned...\")\n",
        "finetuned_deu_ori_metrics = evaluate_model_on_task(finetuned_model, tokenizer, split_datasets['test'], PREFIX_DEU_TO_ORI, GERMAN_LANG_CODE, ODIA_LANG_CODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC4CfG-Vx6mP"
      },
      "outputs": [],
      "source": [
        "# --- DISPLAY FINAL RESULTS TABLE ---\n",
        "results_data = {\n",
        "    'Metric': ['BLEU', 'chrF', 'TER (lower is better)'],\n",
        "    'Odia → German (Baseline)': [baseline_ori_deu_metrics['bleu'], baseline_ori_deu_metrics['chrf'], baseline_ori_deu_metrics['ter']],\n",
        "    'Odia → German (Fine-Tuned)': [finetuned_ori_deu_metrics['bleu'], finetuned_ori_deu_metrics['chrf'], finetuned_ori_deu_metrics['ter']],\n",
        "    'German → Odia (Baseline)': [baseline_deu_ori_metrics['bleu'], baseline_deu_ori_metrics['chrf'], baseline_deu_ori_metrics['ter']],\n",
        "    'German → Odia (Fine-Tuned)': [finetuned_deu_ori_metrics['bleu'], finetuned_deu_ori_metrics['chrf'], finetuned_deu_ori_metrics['ter']],\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(results_data).set_index('Metric')\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"                           FINAL PERFORMANCE RESULTS\")\n",
        "print(\"=\"*80)\n",
        "display(df_results.style.format(\"{:.4f}\"))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZATION: Training and Validation Progress ---\n",
        "\n",
        "print(\"--- Generating Training and Validation Curves ---\")\n",
        "\n",
        "# The trainer object holds the history of the training run\n",
        "# Extract the logs to a pandas DataFrame for easy plotting\n",
        "try:\n",
        "  log_history = trainer.state.log_history\n",
        "  df_logs = pd.DataFrame(log_history)\n",
        "\n",
        "  # Separate training and evaluation logs\n",
        "  df_train = df_logs[df_logs['loss'].notna()].copy()\n",
        "  df_eval = df_logs[df_logs['eval_loss'].notna()].copy()\n",
        "\n",
        "  # Create the plot with two y-axes\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "  fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "  # Plot Training and Validation Loss on the primary y-axis (ax1)\n",
        "  ax1.set_xlabel('Training Step', fontsize=12)\n",
        "  ax1.set_ylabel('Loss', color='C0', fontsize=14)\n",
        "  ax1.plot(df_train['step'], df_train['loss'], color='C0', marker='.', linestyle=':', label='Training Loss')\n",
        "  ax1.plot(df_eval['step'], df_eval['eval_loss'], color='C0', marker='o', linestyle='-', label='Validation Loss')\n",
        "  ax1.tick_params(axis='y', labelcolor='C0')\n",
        "  ax1.legend(loc='upper left')\n",
        "\n",
        "  # Create a second y-axis for the BLEU score\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylabel('Validation BLEU Score', color='C1', fontsize=14)\n",
        "  ax2.plot(df_eval['step'], df_eval['eval_bleu'], color='C1', marker='s', linestyle='--', label='Validation BLEU')\n",
        "  ax2.tick_params(axis='y', labelcolor='C1')\n",
        "  ax2.legend(loc='upper right')\n",
        "\n",
        "  plt.title('Model Training and Validation Progress', fontsize=18, pad=20)\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "except NameError:\n",
        "  print(\"\\n⛔️ ERROR: The 'trainer' object was not found.\")\n",
        "  print(\"Please ensure you have run the main Fine-Tuning cell successfully before running this visualization cell.\")\n",
        "except Exception as e:\n",
        "  print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "r1CBZ9akMXyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart visualizes the model's learning process over the course of the fine-tuning experiment. It provides two key pieces of evidence regarding the success and stability of the training process, directly supporting your methodology.\n",
        "\n",
        "**1. The Proof of Learning: Consistently Decreasing Loss**\n",
        "\n",
        "The most important story on this plot is told by the blue lines, which represent the model's loss.\n",
        "\n",
        "* **Validation Loss (Solid Blue Line):** This is the most critical curve. Its consistent downward trend across the training steps (from an initial value of ~1.32 down to ~1.28) is the definitive indicator that the model was **successfully learning and generalizing**. A decreasing loss on the validation set proves that the model was not simply memorizing the training data but was learning underlying linguistic patterns that allowed it to become more accurate on unseen data. This is strong evidence against overfitting.\n",
        "* **Training Loss (Dotted Blue Line):** The corresponding steep decrease in training loss confirms that the optimization algorithm (`adafactor`) was working correctly, effectively minimizing the error on the data the model was being trained on.\n",
        "\n",
        "Hence, the validation loss consistently decreased throughout the training process, indicating that the model was successfully learning generalizable patterns from the training corpus and was not overfitting. The stable convergence confirms the viability of the chosen hyperparameter configuration and fine-tuning strategy.\n",
        "\n",
        "**2. The Nuanced Finding: A Stable (Flat) Validation BLEU Score**\n",
        "\n",
        "* The BLEU score on the validation set starts high (around 40.5) and remains remarkably stable throughout the training process.\n",
        "* At first glance, a flat BLEU score might seem to contradict the decreasing loss. However, this is a common and insightful finding in NLP. It does not mean the model wasn't learning. It means:\n",
        "\n",
        "1. **High Initial Performance:** The model likely achieved a very good translation quality on the validation set early in the training (by the first evaluation step).\n",
        "\n",
        "2. **Subtle Improvements:** The decreasing validation loss shows the model was becoming progressively more confident in its predictions and was refining the probabilities it assigned to the correct words. However, these improvements in probabilistic accuracy were not significant enough to change the final generated words in a way that would alter the n-gram overlap measured by BLEU. The model was essentially learning to perfect its already good predictions, a subtle improvement that a coarse metric like BLEU may not be sensitive enough to capture epoch-over-epoch.\n",
        "\n",
        "3. **Validation Set Composition:** It's also possible that the validation set, while good, did not contain enough linguistic diversity to reflect these subtle improvements in a changing BLEU score.\n",
        "\n",
        "Hence, an interesting finding from the training process is the stability of the validation BLEU score, which remained high at approximately 40.5 across all evaluation steps. This apparent plateau, when contrasted with the steadily decreasing validation loss, suggests that the model achieved a high level of translation quality early in the fine-tuning process. The subsequent training epochs served to refine and solidify this knowledge, increasing the model's predictive confidence (as shown by the lower loss) without making surface-level changes that would significantly alter the n-gram-based BLEU metric. This highlights a known limitation of discrete evaluation metrics and underscores the value of using validation loss as a primary indicator of successful model convergence. It proves your model learned correctly and didn't overfit."
      ],
      "metadata": {
        "id": "OqyG0ajKo_Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZATION: Score Distribution (KDE Plot) ---\n",
        "\n",
        "print(\"\\n--- Generating Score Distribution Visualizations ---\")\n",
        "\n",
        "# Helper function to generate predictions for this specific analysis\n",
        "def generate_predictions_for_viz(model, tokenizer, inputs, src_lang, tgt_lang, batch_size=8):\n",
        "  \"\"\"\n",
        "  Generates translations for a list of input sentences for visualization or analysis.\n",
        "\n",
        "  This function uses a translation pipeline to generate translations for a list of input sentences,\n",
        "  processing them in batches to manage memory. It is designed for visualization or analysis tasks,\n",
        "  such as inspecting model outputs. Progress is displayed using a `tqdm` progress bar.\n",
        "\n",
        "  Args:\n",
        "    model (object): The translation model (e.g., a Hugging Face model).\n",
        "    tokenizer (object): The tokenizer associated with the model.\n",
        "    inputs (list[str]): A list of input sentences to translate.\n",
        "    src_lang (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang (str): Target language code (e.g., 'deu_Latn').\n",
        "    batch_size (int, optional): Number of sentences to process per batch. Defaults to 8.\n",
        "\n",
        "  Returns:\n",
        "    list[str]: A list of translated sentences.\n",
        "  \"\"\"\n",
        "  translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "  predictions = []\n",
        "  for i in tqdm(range(0, len(inputs), batch_size), desc=f\"Translating for Viz\"):\n",
        "    batch = inputs[i : i + batch_size]\n",
        "    preds = translator(batch, max_length=1200)\n",
        "    predictions.extend([p['translation_text'] for p in preds])\n",
        "  return predictions\n",
        "\n",
        "# --- Generate predictions for both models and both directions ---\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Baseline Predictions\n",
        "print(\"\\nLoading BASELINE model...\")\n",
        "base_model_viz = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "baseline_preds_od = generate_predictions_for_viz(base_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)], ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "baseline_preds_do = generate_predictions_for_viz(base_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)], GERMAN_LANG_CODE, ODIA_LANG_CODE)\n",
        "del base_model_viz\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Fine-Tuned Predictions\n",
        "print(\"\\nLoading FINE-TUNED model...\")\n",
        "finetuned_model_viz = AutoModelForSeq2SeqLM.from_pretrained(FINAL_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\")\n",
        "finetuned_preds_od = generate_predictions_for_viz(finetuned_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)], ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "finetuned_preds_do = generate_predictions_for_viz(finetuned_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)], GERMAN_LANG_CODE, ODIA_LANG_CODE)\n",
        "del finetuned_model_viz\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- Calculate sentence-level BLEU scores ---\n",
        "sbleu = sacrebleu.BLEU(effective_order=True)\n",
        "\n",
        "# For Odia -> German\n",
        "refs_od = [[ex[TARGET_FIELD]] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)]\n",
        "baseline_scores_od = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(baseline_preds_od, refs_od)]\n",
        "finetuned_scores_od = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(finetuned_preds_od, refs_od)]\n",
        "df_dist_od = pd.DataFrame({'Baseline': baseline_scores_od, 'Fine-Tuned': finetuned_scores_od})\n",
        "\n",
        "# For German -> Odia\n",
        "refs_do = [[ex[TARGET_FIELD]] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)]\n",
        "baseline_scores_do = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(baseline_preds_do, refs_do)]\n",
        "finetuned_scores_do = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(finetuned_preds_do, refs_do)]\n",
        "df_dist_do = pd.DataFrame({'Baseline': baseline_scores_do, 'Fine-Tuned': finetuned_scores_do})\n",
        "\n",
        "# --- Create the plots ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig.suptitle('Distribution of Sentence-Level BLEU Scores by Translation Direction', fontsize=18, y=1.02)\n",
        "\n",
        "# Plot 1: Odia -> German\n",
        "sns.kdeplot(data=df_dist_od, x='Baseline', fill=True, label='Baseline', clip=(0, 100), lw=3, ax=axes[0], color='C0')\n",
        "sns.kdeplot(data=df_dist_od, x='Fine-Tuned', fill=True, label='Fine-Tuned', clip=(0, 100), lw=3, ax=axes[0], color='C1')\n",
        "axes[0].set_title('Odia → German', fontsize=14)\n",
        "axes[0].set_xlabel('Sentence BLEU Score', fontsize=12)\n",
        "axes[0].set_ylabel('Density', fontsize=12)\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: German -> Odia\n",
        "sns.kdeplot(data=df_dist_do, x='Baseline', fill=True, label='Baseline', clip=(0, 100), lw=3, ax=axes[1], color='C0')\n",
        "sns.kdeplot(data=df_dist_do, x='Fine-Tuned', fill=True, label='Fine-Tuned', clip=(0, 100), lw=3, ax=axes[1], color='C1')\n",
        "axes[1].set_title('German → Odia', fontsize=14)\n",
        "axes[1].set_xlabel('Sentence BLEU Score', fontsize=12)\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZwrAEKkmM4lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLuaEx6Ix9uc"
      },
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "# print(\"\\nSaving the final bidirectional model...\")\n",
        "# trainer.save_model(FINAL_MODEL_PATH)\n",
        "# print(f\"Model saved to '{FINAL_MODEL_PATH}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtC2JwdiVVI"
      },
      "source": [
        "## Visualization: Final Performance\n",
        "\n",
        "To compare the final scores of the baseline vs. fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE0icz3yiVuF"
      },
      "outputs": [],
      "source": [
        "# --- 1. CONSOLIDATE RESULTS DATA ---\n",
        "plot_data = [\n",
        "    # --- Odia -> German Data ---\n",
        "    {'Metric': 'BLEU', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['ter']},\n",
        "\n",
        "    {'Metric': 'BLEU', 'Direction': 'Odia → German', 'Model': 'Fine-Tuned', 'Score': finetuned_ori_deu_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'Odia → German', 'Model': 'Fine-Tuned', 'Score': finetuned_ori_deu_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'Odia → German', 'Model': 'Fine-Tuned', 'Score': finetuned_ori_deu_metrics['ter']},\n",
        "\n",
        "    # --- German -> Odia Data ---\n",
        "    {'Metric': 'BLEU', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['ter']},\n",
        "\n",
        "    {'Metric': 'BLEU', 'Direction': 'German → Odia', 'Model': 'Fine-Tuned', 'Score': finetuned_deu_ori_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'German → Odia', 'Model': 'Fine-Tuned', 'Score': finetuned_deu_ori_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'German → Odia', 'Model': 'Fine-Tuned', 'Score': finetuned_deu_ori_metrics['ter']},\n",
        "]\n",
        "\n",
        "df_plot = pd.DataFrame(plot_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd-51E8xibx1"
      },
      "outputs": [],
      "source": [
        "# --- 2. CREATE THE VISUALIZATION ---\n",
        "\n",
        "# Set the theme\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Create a figure and a set of subplots. This gives us explicit control.\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Define the two directions for the loop\n",
        "directions = ['Odia → German', 'German → Odia']\n",
        "palettes = ['viridis', 'plasma']\n",
        "\n",
        "# Loop through the two axes and directions to create the plots\n",
        "for i, direction in enumerate(directions):\n",
        "  # Filter data for the current subplot\n",
        "  subset_data = df_plot[df_plot['Direction'] == direction]\n",
        "\n",
        "  # Create the bar plot on the specific axis (axes[i])\n",
        "  sns.barplot(\n",
        "      data=subset_data,\n",
        "      x='Metric',\n",
        "      y='Score',\n",
        "      hue='Model', # 'Model' is responsible for the color distinction (Baseline vs Fine-tuned)\n",
        "      palette=palettes[i],\n",
        "      ax=axes[i]\n",
        "  )\n",
        "\n",
        "  # Set titles and labels for each subplot\n",
        "  axes[i].set_title(f'Performance for {direction}', fontsize=15)\n",
        "  axes[i].set_xlabel('')\n",
        "  axes[i].set_ylabel('Score', fontsize=12)\n",
        "  axes[i].tick_params(axis='x', labelsize=12)\n",
        "\n",
        "  # Add the value annotations on top of the bars for each subplot\n",
        "  for p in axes[i].patches:\n",
        "    axes[i].annotate(f'{p.get_height():.2f}',\n",
        "     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                     ha = 'center', va = 'center',\n",
        "                     xytext = (0, 9),\n",
        "                     textcoords = 'offset points',\n",
        "                     fontsize=11, fontweight='bold')\n",
        "\n",
        "  # Add legend for the current subplot\n",
        "  axes[i].legend(title='Model Version', loc='upper right')\n",
        "\n",
        "\n",
        "# Set a main title for the entire figure\n",
        "fig.suptitle('Translation Performance: Baseline vs. Fine-Tuned Model', fontsize=20, y=1.03)\n",
        "\n",
        "# Add the footnote text below the plots\n",
        "fig.text(0.5, 0.01, '*For TER (Translation Edit Rate), a lower score is better.', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "# Adjust layout to prevent labels/titles from overlapping\n",
        "plt.tight_layout(rect=[0, 0.05, 1, 0.98]) # Adjust rect to make space for suptitle and footnote\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOG FINAL RESULTS TO wandb\n",
        "\n",
        "This code will take `df_results` DataFrame and log it as a beautiful, interactive table in wandb dashboard."
      ],
      "metadata": {
        "id": "3wlckD11UpMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "print(\"Initializing a new wandb run to log final results...\")\n",
        "\n",
        "wandb.init(\n",
        "    project=\"Odia-German-Thesis\",  # Groups all runs into a single project\n",
        "    name=\"Final Performance Analysis\", # Give this specific run a descriptive name\n",
        "    job_type=\"evaluation\"          # Categorize the run type\n",
        ")\n",
        "\n",
        "\n",
        "# LOG THE DATA\n",
        "print(\"\\nLogging final comparison results to Weights & Biases...\")\n",
        "\n",
        "# Create a wandb.Table object from results DataFrame\n",
        "# We reset the index to make 'Metric' a column in the table\n",
        "wandb_table = wandb.Table(dataframe=df_results.reset_index())\n",
        "\n",
        "# Log the interactive table to new run\n",
        "wandb.log({\"final_performance_comparison\": wandb_table})\n",
        "\n",
        "# It's also good practice to update the run's summary with key final metrics\n",
        "# This makes them easy to see and compare on the project dashboard\n",
        "wandb.summary[\"final_bleu_ori_deu_baseline\"] = baseline_ori_deu_metrics['bleu']\n",
        "wandb.summary[\"final_bleu_ori_deu_finetuned\"] = finetuned_ori_deu_metrics['bleu']\n",
        "wandb.summary[\"final_bleu_deu_ori_baseline\"] = baseline_deu_ori_metrics['bleu']\n",
        "wandb.summary[\"final_bleu_deu_ori_finetuned\"] = finetuned_deu_ori_metrics['bleu']\n",
        "\n",
        "# FINISH THE RUN\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\n✅ Final results successfully logged to your W&B dashboard.\")"
      ],
      "metadata": {
        "id": "gLf6Ob6jVL80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "cER29AITSwKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to the folder containing all the saved files\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/nllb-odia-german-translator_model_final\"\n",
        "\n",
        "# LOAD THE FINE-TUNED MODEL AND TOKENIZER\n",
        "print(\"Loading the fine-tuned tokenizer and model...\")\n",
        "# Load the fine-tuned tokenizer and model from the directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n",
        "\n",
        "print(\"✅ Model and Tokenizer loaded successfully.\")\n",
        "\n",
        "# CREATE THE TRANSLATION PIPELINE\n",
        "# Provide default language codes during initialization to satisfy the pipeline's requirement.\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    src_lang=ODIA_LANG_CODE,\n",
        "    tgt_lang=GERMAN_LANG_CODE,\n",
        ")\n",
        "\n",
        "print(\"✅ Translation pipeline created.\")"
      ],
      "metadata": {
        "id": "W8XsRYCGSvaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERFORM TRANSLATION\n",
        "# --- Example 1: Odia to German (The Default Direction) ---\n",
        "odia_input_text = \"ଆଜି ପାଗ ବହୁତ ଭଲ ଅଛି।\"\n",
        "full_input_od = PREFIX_ORI_TO_DEU + odia_input_text\n",
        "\n",
        "print(f\"\\nTranslating (Odia → German): '{odia_input_text}'\")\n",
        "\n",
        "german_translation = translator(full_input_od, max_length=128)\n",
        "print(f\"Output: {german_translation[0]['translation_text']}\")"
      ],
      "metadata": {
        "id": "qjD4ZW9dacR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example 2: German to Odia (Overriding the Default Direction) ---\n",
        "german_input_text = \"Wie ist das Wetter heute?\"\n",
        "full_input_de = PREFIX_DEU_TO_ORI + german_input_text\n",
        "\n",
        "print(f\"\\nTranslating (German → Odia): '{german_input_text}'\")"
      ],
      "metadata": {
        "id": "M1vnA-aodOWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To translate in the reverse direction, override the languages directly in the call.\n",
        "odia_translation = translator(\n",
        "    full_input_de,\n",
        "    src_lang=GERMAN_LANG_CODE,\n",
        "    tgt_lang=ODIA_LANG_CODE,\n",
        "    max_length=128\n",
        ")\n",
        "print(f\"Output: {odia_translation[0]['translation_text']}\")"
      ],
      "metadata": {
        "id": "jlXpeQ8ceAI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svnLg2ODSKLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}