{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Uploader for Hugging Face Hub\n",
        "## Objective\n",
        "The sole objective of this notebook is to take a final, trained translation model from its saved directory in Google Drive and publish it to a new repository on the Hugging Face Hub. This is a critical deployment step that makes the model artifacts centrally available and ready for inference by other applications, such as a Gradio web interface.\n",
        "\n",
        "## Methodology\n",
        "The script uses the `huggingface_hub` library to programmatically interact with the Hugging Face platform. It automates the following steps:\n",
        "\n",
        "1. **Authentication:** Securely logs into a Hugging Face account using an access token.\n",
        "2. **Repository Creation:** Creates a new, public model repository under the specified user account.\n",
        "3. **File Upload:** Uploads the entire contents of the local model directory—including the model weights, configuration files, and tokenizer files—to the Hub repository.\n",
        "\n",
        "## Workflow\n",
        "1. Mounts Google Drive to access the saved model files.\n",
        "2. Installs the huggingface_hub library.\n",
        "3. Prompts the user to log in to their Hugging Face account.\n",
        "4. Creates a new repository on the Hub with a specified name.\n",
        "5. Uploads all files from the local model path to the Hub repository.\n",
        "\n",
        "## Input & Output\n",
        "* **Input:** A folder in Google Drive containing the saved artifacts of a fine-tuned model (e.g., `lora-odia-german-translator`).\n",
        "* **Output:** A new, public model repository on the Hugging Face Hub containing all the uploaded model files."
      ],
      "metadata": {
        "id": "tfA0v8bZgUz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "1nUlvET4kuzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8mPNBl6gGF4"
      },
      "outputs": [],
      "source": [
        "# Install the library to interact with the Hub\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from huggingface_hub import HfApi\n",
        "import os"
      ],
      "metadata": {
        "id": "3M689nhpgPEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Hugging Face\n",
        "from google.colab import userdata\n",
        "huggingface_token = userdata.get('HF_TOKEN')\n",
        "login(token=huggingface_token)"
      ],
      "metadata": {
        "id": "wEzeV2zXgR0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Hugging Face username\n",
        "HF_USERNAME = \"abhinandansamal\"\n",
        "\n",
        "# The name of the model on the Hub\n",
        "# MODEL_HUB_NAME = \"nllb-200-distilled-600M-finetuned-odia-german-bidirectional\"  # Full fine-tuned model\n",
        "MODEL_HUB_NAME = \"nllb-200-distilled-600M-LoRA-finetuned-odia-german-bidirectional\"  # Adapter-based fine-tuned model\n",
        "\n",
        "# The local path in Google Drive where the final model is saved\n",
        "# LOCAL_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/nllb-odia-german-translator_model_final\"  # Full fine-tuned model local path\n",
        "LOCAL_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/lora-odia-german-translator\"  # Adapter-based fine-tuned model local path\n",
        "\n",
        "# --- Initialize HuggingFace API ---\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "B_rjLFE0gUYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new repository on the Hub\n",
        "repo_url = api.create_repo(\n",
        "    repo_id=f\"{HF_USERNAME}/{MODEL_HUB_NAME}\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True\n",
        ")\n",
        "print(f\"Model repository created at: {repo_url}\")"
      ],
      "metadata": {
        "id": "Geg85tr7mYut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the entire folder containing your model files\n",
        "print(f\"Uploading files from {LOCAL_MODEL_PATH}...\")\n",
        "api.upload_folder(\n",
        "    folder_path=LOCAL_MODEL_PATH,\n",
        "    repo_id=f\"{HF_USERNAME}/{MODEL_HUB_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Your model has been successfully uploaded to the Hugging Face Hub!\")\n",
        "print(f\"You can view it here: {repo_url}\")"
      ],
      "metadata": {
        "id": "xpjb6d5AnN5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4mTHiX8neRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}