{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comparative Analysis of NLLB Fine-Tuning Methodologies\n",
        "## Objective\n",
        "The objective of this notebook is to serve as the final experimental and analysis hub for the thesis. It conducts the **Adapter-Based (LoRA) Fine-Tuning** experiment and then performs a comprehensive, multi-metric comparative analysis between the **Baseline**, the **Fully Fine-Tuned**, and the newly trained **LoRA-Tuned** models. The script's final outputs are the definitive tables and visualizations that compare the performance and efficiency of all three approaches.\n",
        "\n",
        "## Methodology\n",
        "The notebook follows a rigorous, multi-stage experimental design:\n",
        "\n",
        "1. **Data Loading:** Loads the pre-split and tokenized datasets for training, validation, and testing.\n",
        "2. **Baseline Evaluation:** Establishes a performance benchmark by evaluating the original, pre-trained NLLB model on the held-out test set.\n",
        "3. **Adapter-Based (LoRA) Fine-Tuning:** Loads the base NLLB model in 8-bit precision, injects trainable LoRA adapters, and then trains only these lightweight adapters.\n",
        "4. **Final Evaluation:** Merges the trained adapters with the base model and evaluates the resulting model on the same held-out test set.\n",
        "5. **Results Synthesis:** Crucially, it consolidates the new metrics from the Baseline and LoRA models with the pre-computed results from the Full Fine-Tuning experiment. It then generates a final comparison table and visualizations showing the performance of all three models.\n",
        "\n",
        "## Workflow\n",
        "1. Mounts Google Drive for data access.\n",
        "2. Installs all required libraries, including `peft`.\n",
        "3. Loads the pre-processed and tokenized datasets.\n",
        "4. Performs the baseline evaluation.\n",
        "5. Executes the Adapter-Based (LoRA) Fine-Tuning training run.\n",
        "6. Performs the final evaluation on the adapter-tuned model.\n",
        "7. Combines all results and generates the final summary table and visualizations comparing all three models.\n",
        "\n",
        "## Input & Output\n",
        "* **Input:** Pre-processed and tokenized training, validation, and test `DatasetDict` objects. It also implicitly uses the pre-computed scores from the Full Fine-Tuning experiment for the final comparison.\n",
        "* **Output:**\n",
        "  * Saved LoRA adapter artifacts in a specified Google Drive directory.\n",
        "  * A printed summary table comparing the performance of the **Baseline**, **Fully Fine-Tuned**, and **LoRA-Tuned models**.\n",
        "  * A series of plots visualizing the final performance comparison and training dynamics for the LoRA model."
      ],
      "metadata": {
        "id": "DJnDs1qt0kwh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBO56nwzxvwC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets sacrebleu torch accelerate pandas bitsandbytes seaborn matplotlib peft"
      ],
      "metadata": {
        "id": "WS_1aKG6x0O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- All Installed Packages (pip list) ---\")\n",
        "!pip list"
      ],
      "metadata": {
        "id": "TwOsKTJ-1zJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "92qPH6xEyAh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "CORPUS_FILE = \"/content/drive/MyDrive/Thesis/data/transformed/bidirectional_corpus_final.jsonl\"\n",
        "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "LORA_OUTPUT_DIR = \"/content/drive/MyDrive/Thesis/eval/lora-finetuned-odia-german-results\"\n",
        "FINAL_LORA_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/lora-odia-german-translator\"\n",
        "\n",
        "INPUT_FIELD = \"input_text\"\n",
        "TARGET_FIELD = \"target_text\"\n",
        "ODIA_LANG_CODE = \"ory_Orya\"\n",
        "GERMAN_LANG_CODE = \"deu_Latn\"\n",
        "TEST_SET_SIZE = 0.10\n",
        "VALIDATION_SET_SIZE = 0.10\n",
        "PREFIX_ORI_TO_DEU = \"translate Odia to German: \"\n",
        "PREFIX_DEU_TO_ORI = \"translate German to Odia: \""
      ],
      "metadata": {
        "id": "-8ivQMSiyEBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DATA LOADING AND PREPARATION ---\n",
        "print(\"Loading the rich bidirectional dataset...\")\n",
        "\n",
        "# --- Manually load the JSONL file into a list of dictionaries ---\n",
        "data_list = []\n",
        "with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data_list.append(json.loads(line))\n",
        "\n",
        "# --- Create a pandas DataFrame ---\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# --- Convert the DataFrame into a Hugging Face Dataset object ---\n",
        "# This bypasses the caching issue.\n",
        "full_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# --- Shuffle the dataset before splitting ---\n",
        "full_dataset = full_dataset.shuffle(seed=42)\n",
        "\n",
        "# --- Create the Test Split ---\n",
        "# This first split separates the final, unseen test set.\n",
        "train_valid_split = full_dataset.train_test_split(test_size=TEST_SET_SIZE, seed=42)\n",
        "test_dataset = train_valid_split['test']\n",
        "\n",
        "# --- Create the Train and Validation Splits ---\n",
        "# The remaining data is split again to create the training and validation sets.\n",
        "train_val_split = train_valid_split['train'].train_test_split(test_size=VALIDATION_SET_SIZE / (1 - TEST_SET_SIZE), seed=42)\n",
        "train_dataset = train_val_split['train']\n",
        "validation_dataset = train_val_split['test']\n",
        "\n",
        "# Combine into a single DatasetDict for convenience\n",
        "split_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': validation_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(\"\\nDataset successfully split:\")\n",
        "print(f\"Training set size: {len(split_datasets['train'])}\")\n",
        "print(f\"Validation set size: {len(split_datasets['validation'])}\")\n",
        "print(f\"Test set size: {len(split_datasets['test'])}\")"
      ],
      "metadata": {
        "id": "h7lH599xyMds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TOKENIZATION ---\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=ODIA_LANG_CODE, tgt_lang=GERMAN_LANG_CODE)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  \"\"\"\n",
        "  Tokenizes input and target text fields for model training or evaluation.\n",
        "\n",
        "  This function processes a batch of examples from a dataset, tokenizing the `input_text` and\n",
        "  `target_text` fields using a provided tokenizer. It applies truncation to a maximum length of\n",
        "  1200 tokens and generates input IDs for both inputs and labels, with the latter prepared in the\n",
        "  tokenizer's target context. The resulting dictionary includes tokenized inputs and labels suitable\n",
        "  for training or evaluating a translation model.\n",
        "\n",
        "  Args:\n",
        "    examples (dict): A dictionary containing lists of input and target sentences, with keys\n",
        "                     defined by `INPUT_FIELD` and `TARGET_FIELD` (e.g., 'input_text' and 'target_text').\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing tokenized inputs (`input_ids`, etc.) and labels\n",
        "          (`labels` as input IDs from target text tokenization).\n",
        "\n",
        "  Note:\n",
        "    - Assumes global variables `INPUT_FIELD`, `TARGET_FIELD`, and `tokenizer` are defined.\n",
        "    - Requires a tokenizer compatible with Hugging Face's `transformers` library.\n",
        "    - The maximum length of 1200 tokens is hardcoded for truncation.\n",
        "\n",
        "  Example:\n",
        "    >>> from transformers import AutoTokenizer\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "    >>> INPUT_FIELD = \"input_text\"\n",
        "    >>> TARGET_FIELD = \"target_text\"\n",
        "    >>> examples = {\n",
        "    ...     \"input_text\": [\"ନମସ୍କାର\"],\n",
        "    ...     \"target_text\": [\"Hallo\"]\n",
        "    ... }\n",
        "    >>> result = preprocess_function(examples)\n",
        "    >>> print(result.keys())\n",
        "    dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
        "  \"\"\"\n",
        "  # Access the lists of sentences directly from the batch columns\n",
        "  inputs = examples[INPUT_FIELD]\n",
        "  targets = examples[TARGET_FIELD]\n",
        "\n",
        "  model_inputs = tokenizer(inputs, max_length=1200, truncation=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(targets, max_length=1200, truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "\n",
        "print(\"Tokenizing all dataset splits...\")\n",
        "# Note: The `remove_columns` is correct because we are removing the original columns from the DataFrame\n",
        "tokenized_datasets = split_datasets.map(preprocess_function, batched=True, batch_size=16, remove_columns=split_datasets['train'].column_names)\n",
        "print(\"Tokenization complete.\")"
      ],
      "metadata": {
        "id": "bI09tuhryQVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BASELINE EVALUATION ---\n",
        "def evaluate_model_on_task(model_to_eval, tokenizer, test_data, task_prefix, src_lang, tgt_lang, batch_size=8):\n",
        "  \"\"\"\n",
        "  Evaluates a translation model on a specific translation direction using a test dataset.\n",
        "\n",
        "  This function filters test data for sentences starting with a specified task prefix, generates\n",
        "  translations using a translation pipeline, and computes BLEU, CHRF++, and TER scores against\n",
        "  reference translations. It processes the data in batches to manage memory and displays progress\n",
        "  using a `tqdm` progress bar. If no data matches the prefix, it returns zero scores.\n",
        "\n",
        "  Args:\n",
        "    model_to_eval (object): The translation model to evaluate (e.g., a Hugging Face model).\n",
        "    tokenizer (object): The tokenizer associated with the model.\n",
        "    test_data (list[dict]): A list of dictionaries containing input and target sentences,\n",
        "                            with keys defined by `INPUT_FIELD` and `TARGET_FIELD`.\n",
        "    task_prefix (str): A prefix to filter test data for a specific translation direction (e.g., '[ORI_TO_DEU]').\n",
        "    src_lang (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang (str): Target language code (e.g., 'deu_Latn').\n",
        "    batch_size (int, optional): Number of sentences to process per batch. Defaults to 8.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing evaluation metrics:\n",
        "          - 'bleu': BLEU score.\n",
        "          - 'chrf': CHRF++ score (word order 2).\n",
        "          - 'ter': TER score.\n",
        "  \"\"\"\n",
        "  # Filter the test set for the specific task using the full prefix\n",
        "  task_inputs = [ex[INPUT_FIELD] for ex in test_data if ex[INPUT_FIELD].startswith(task_prefix)]\n",
        "  task_references = [[ex[TARGET_FIELD]] for ex in test_data if ex[INPUT_FIELD].startswith(task_prefix)]\n",
        "\n",
        "  if not task_inputs:\n",
        "    print(f\"Warning: No test data found for prefix '{task_prefix}'\")\n",
        "    return {\"bleu\": 0, \"chrf\": 0, \"ter\": 0}\n",
        "\n",
        "  device = 0 if torch.cuda.is_available() else -1\n",
        "  translator = pipeline(\"translation\", model=model_to_eval, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "  decoded_preds = []\n",
        "  print(f\"Generating translations for {len(task_inputs)} sentences in batches of {batch_size}...\")\n",
        "  for i in tqdm(range(0, len(task_inputs), batch_size)):\n",
        "    batch = task_inputs[i : i + batch_size]\n",
        "    predictions = translator(batch, max_length=1200)\n",
        "    decoded_preds.extend([pred['translation_text'] for pred in predictions])\n",
        "\n",
        "  bleu = sacrebleu.BLEU().corpus_score(decoded_preds, task_references).score\n",
        "  chrf = sacrebleu.CHRF(word_order=2).corpus_score(decoded_preds, task_references).score\n",
        "  ter = sacrebleu.TER().corpus_score(decoded_preds, task_references).score\n",
        "\n",
        "  return {\"bleu\": bleu, \"chrf\": chrf, \"ter\": ter}"
      ],
      "metadata": {
        "id": "zGM5_kchyTbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Evaluating Baseline Model ---\")\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Evaluating Odia -> German baseline...\")\n",
        "baseline_ori_deu_metrics = evaluate_model_on_task(base_model, tokenizer, split_datasets['test'], PREFIX_ORI_TO_DEU, ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "\n",
        "print(\"Evaluating German -> Odia baseline...\")\n",
        "baseline_deu_ori_metrics = evaluate_model_on_task(base_model, tokenizer, split_datasets['test'], PREFIX_DEU_TO_ORI, GERMAN_LANG_CODE, ODIA_LANG_CODE)"
      ],
      "metadata": {
        "id": "Q9ntdFtfyl0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADAPTER-BASED (LoRA) FINE-TUNING ---\n",
        "print(\"\\n--- Starting Adapter-Based (LoRA) Fine-Tuning Process ---\")\n",
        "\n",
        "# Load the base model (load it in 8-bit to save memory even during training)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # Rank of the update matrices. Higher means more parameters, potentially more expressive.\n",
        "    lora_alpha=32, # LoRA scaling factor.\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Apply LoRA to the query and value projections in attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "# Create the PEFT model by wrapping the base model with the LoRA config\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters() # This will show how few parameters we are actually training!\n",
        "\n",
        "# Define metric computation function (can be simpler for LoRA validation)\n",
        "def compute_metrics_for_trainer(eval_preds):\n",
        "  \"\"\"\n",
        "  Computes BLEU score for model evaluation during training.\n",
        "\n",
        "  This function processes model predictions and labels from a trainer's evaluation step,\n",
        "  decodes them using a tokenizer, and calculates the BLEU score using the `sacrebleu` library.\n",
        "  It handles padding tokens and special tokens to ensure accurate metric computation.\n",
        "\n",
        "  Args:\n",
        "    eval_preds (tuple): A tuple containing predictions and labels from the trainer.\n",
        "                        Predictions are token IDs, and labels are token IDs with -100 for ignored positions.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing the BLEU score with key 'bleu'.\n",
        "\n",
        "  Note:\n",
        "    - Assumes a global `tokenizer` variable with a `batch_decode` method and `pad_token_id`.\n",
        "    - Requires `sacrebleu` and `numpy` libraries.\n",
        "    - Designed for use with Hugging Face's `transformers` Trainer API.\n",
        "\n",
        "  Example:\n",
        "    >>> from transformers import AutoTokenizer\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "    >>> eval_preds = (\n",
        "    ...     tokenizer([\"Hallo\"], return_tensors=\"pt\")[\"input_ids\"].numpy(),\n",
        "    ...     tokenizer([\"Hallo\"], return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
        "    ... )\n",
        "    >>> result = compute_metrics_for_trainer(eval_preds)\n",
        "    >>> print(result)\n",
        "    {'bleu': 100.0}\n",
        "  \"\"\"\n",
        "  preds, labels = eval_preds\n",
        "  if isinstance(preds, tuple): preds = preds[0]\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "  decoded_labels = [[label] for label in decoded_labels]\n",
        "  result = sacrebleu.BLEU().corpus_score(decoded_preds, decoded_labels)\n",
        "  return {\"bleu\": result.score}\n",
        "\n",
        "# Define Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=LORA_OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-3,\n",
        "    # LoRA is memory-efficient, so we can often use a larger physical batch size\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=5,      # More epochs are fine as it's faster and less prone to overfitting\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize and run the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        "    compute_metrics=compute_metrics_for_trainer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EK1q1ub4y8KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step G: Save the trained LoRA adapters\n",
        "print(\"\\nSaving the fine-tuned LoRA adapters...\")\n",
        "trainer.save_model(FINAL_LORA_MODEL_PATH)\n",
        "tokenizer.save_pretrained(FINAL_LORA_MODEL_PATH) # Also save the tokenizer for easy loading\n",
        "print(f\"Adapters saved to '{FINAL_LORA_MODEL_PATH}'\")"
      ],
      "metadata": {
        "id": "sq4KpDkVzfqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL EVALUATION OF THE ADAPTER MODEL ---\n",
        "print(\"\\n--- Evaluating Fine-Tuned Adapter Model ---\")\n",
        "\n",
        "# Step A: Load the base model in 8-bit again\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Step B: Load the LoRA adapters and merge them into the base model\n",
        "# This creates a full model in memory for inference\n",
        "adapter_model = PeftModel.from_pretrained(base_model, FINAL_LORA_MODEL_PATH)\n",
        "adapter_model.eval() # Set to evaluation mode\n",
        "\n",
        "# Step C: Run evaluation using your existing evaluation function\n",
        "print(\"Evaluating Odia -> German with Adapter-Tuned Model...\")\n",
        "adapter_ori_deu_metrics = evaluate_model_on_task(adapter_model, tokenizer, split_datasets['test'], PREFIX_ORI_TO_DEU, ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "\n",
        "print(\"Evaluating German -> Odia with Adapter-Tuned Model...\")\n",
        "adapter_deu_ori_metrics = evaluate_model_on_task(adapter_model, tokenizer, split_datasets['test'], PREFIX_DEU_TO_ORI, GERMAN_LANG_CODE, ODIA_LANG_CODE)"
      ],
      "metadata": {
        "id": "ZsO8CuSTzpeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear memory after evaluation\n",
        "del base_model, adapter_model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8jskptRO0qL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DISPLAY FINAL RESULTS ---\n",
        "\n",
        "# manually insert the results from full fine-tuning experiment for a complete comparison\n",
        "full_finetune_ori_deu_metrics = {\"bleu\": 65.1630, \"chrf\": 78.9527, \"ter\": 39.3919}\n",
        "full_finetune_deu_ori_metrics = {\"bleu\": 21.2164, \"chrf\": 48.5377, \"ter\": 77.4971}\n",
        "\n",
        "# Consolidate all results for the final table and plots\n",
        "results_data = {\n",
        "    'Metric': ['BLEU', 'chrF', 'TER (lower is better)'],\n",
        "    'Baseline': [baseline_ori_deu_metrics['bleu'], baseline_ori_deu_metrics['chrf'], baseline_ori_deu_metrics['ter']],\n",
        "    'Full Fine-Tune': [full_finetune_ori_deu_metrics['bleu'], full_finetune_ori_deu_metrics['chrf'], full_finetune_ori_deu_metrics['ter']],\n",
        "    'Adapter (LoRA) Fine-Tune': [adapter_ori_deu_metrics['bleu'], adapter_ori_deu_metrics['chrf'], adapter_ori_deu_metrics['ter']],\n",
        "}\n",
        "df_results_od = pd.DataFrame(results_data).set_index('Metric')\n",
        "\n",
        "results_data_do = {\n",
        "    'Metric': ['BLEU', 'chrF', 'TER (lower is better)'],\n",
        "    'Baseline': [baseline_deu_ori_metrics['bleu'], baseline_deu_ori_metrics['chrf'], baseline_deu_ori_metrics['ter']],\n",
        "    'Full Fine-Tune': [full_finetune_deu_ori_metrics['bleu'], full_finetune_deu_ori_metrics['chrf'], full_finetune_deu_ori_metrics['ter']],\n",
        "    'Adapter (LoRA) Fine-Tune': [adapter_deu_ori_metrics['bleu'], adapter_deu_ori_metrics['chrf'], adapter_deu_ori_metrics['ter']],\n",
        "}\n",
        "df_results_do = pd.DataFrame(results_data_do).set_index('Metric')\n",
        "\n",
        "\n",
        "# --- Display the Final Comparison Tables ---\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"             FINAL PERFORMANCE COMPARISON: ODIA → GERMAN\")\n",
        "print(\"=\"*80)\n",
        "display(df_results_od.style.format(\"{:.4f}\"))\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"             FINAL PERFORMANCE COMPARISON: GERMAN → ODIA\")\n",
        "print(\"=\"*80)\n",
        "display(df_results_do.style.format(\"{:.4f}\"))\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "W9VEX0_50rA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate Final Comparison Visualization ---\n",
        "print(\"\\n--- Generating Final Performance Visualization ---\")\n",
        "\n",
        "# --- CONSOLIDATE RESULTS DATA ---\n",
        "plot_data = [\n",
        "    # Odia -> German Data\n",
        "    {'Metric': 'BLEU', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'Odia → German', 'Model': 'Baseline', 'Score': baseline_ori_deu_metrics['ter']},\n",
        "    {'Metric': 'BLEU', 'Direction': 'Odia → German', 'Model': 'Full Fine-Tune', 'Score': full_finetune_ori_deu_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'Odia → German', 'Model': 'Full Fine-Tune', 'Score': full_finetune_ori_deu_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'Odia → German', 'Model': 'Full Fine-Tune', 'Score': full_finetune_ori_deu_metrics['ter']},\n",
        "    {'Metric': 'BLEU', 'Direction': 'Odia → German', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_ori_deu_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'Odia → German', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_ori_deu_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'Odia → German', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_ori_deu_metrics['ter']},\n",
        "\n",
        "    # German -> Odia Data\n",
        "    {'Metric': 'BLEU', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'German → Odia', 'Model': 'Baseline', 'Score': baseline_deu_ori_metrics['ter']},\n",
        "    {'Metric': 'BLEU', 'Direction': 'German → Odia', 'Model': 'Full Fine-Tune', 'Score': full_finetune_deu_ori_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'German → Odia', 'Model': 'Fine-Tuned', 'Score': full_finetune_deu_ori_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'German → Odia', 'Model': 'Full Fine-Tune', 'Score': full_finetune_deu_ori_metrics['ter']},\n",
        "    {'Metric': 'BLEU', 'Direction': 'German → Odia', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_deu_ori_metrics['bleu']},\n",
        "    {'Metric': 'chrF', 'Direction': 'German → Odia', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_deu_ori_metrics['chrf']},\n",
        "    {'Metric': 'TER', 'Direction': 'German → Odia', 'Model': 'Adapter (LoRA) Fine-Tune', 'Score': adapter_deu_ori_metrics['ter']},\n",
        "]\n",
        "\n",
        "# The model name in the results dictionary is 'Fine-Tuned', but for the legend let's rename it\n",
        "for item in plot_data:\n",
        "  if item['Model'] == 'Fine-Tuned':\n",
        "    item['Model'] = 'Full Fine-Tune'\n",
        "\n",
        "final_plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "\n",
        "# --- CREATE THE VISUALIZATION ---\n",
        "\n",
        "# Set the theme\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# This ensures \"Baseline\" is always the same color, \"Full Fine-Tune\" is always another, etc.\n",
        "palette_dict = {\n",
        "    \"Baseline\": \"C0\", # Standard blue\n",
        "    \"Full Fine-Tune\": \"C1\", # Standard orange\n",
        "    \"Adapter (LoRA) Fine-Tune\": \"C2\" # Standard green\n",
        "}\n",
        "\n",
        "# Create a figure and a set of subplots side-by-side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Define the two directions for the loop\n",
        "directions = ['Odia → German', 'German → Odia']\n",
        "\n",
        "# Loop through the two axes and directions to create the plots\n",
        "for i, direction in enumerate(directions):\n",
        "  ax = axes[i]\n",
        "  subset_data = final_plot_df[final_plot_df['Direction'] == direction]\n",
        "\n",
        "  # Create the bar plot on the specific axis, using the consistent palette\n",
        "  sns.barplot(\n",
        "      data=subset_data,\n",
        "      x='Metric',\n",
        "      y='Score',\n",
        "      hue='Model',\n",
        "      palette=palette_dict,\n",
        "      ax=ax\n",
        "  )\n",
        "\n",
        "  ax.set_title(f'Performance for {direction}', fontsize=16)\n",
        "  ax.set_xlabel('')\n",
        "  ax.set_ylabel('Score', fontsize=14)\n",
        "  ax.tick_params(axis='x', labelsize=12)\n",
        "  ax.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "  # Add value annotations on top of the bars\n",
        "  for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}',\n",
        "     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9), textcoords='offset points',\n",
        "                fontsize=11, fontweight='bold')\n",
        "\n",
        "# --- FINALIZE THE PLOT ---\n",
        "\n",
        "# Add a single main title for the entire figure\n",
        "fig.suptitle('Translation Performance Comparison by Direction and Method', fontsize=22, y=1.0)\n",
        "\n",
        "# Get handles and labels from one of the plots\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "\n",
        "# Remove the automatically generated legends from the subplots\n",
        "axes[0].get_legend().remove()\n",
        "axes[1].get_legend().remove()\n",
        "\n",
        "# Adjust the subplot layout to make space on the right for the legend\n",
        "fig.subplots_adjust(right=0.85, top=0.90)\n",
        "\n",
        "# Create a single, clean legend for the entire figure in the new empty space\n",
        "fig.legend(\n",
        "    handles,\n",
        "    labels,\n",
        "    loc='center left',\n",
        "    title='Fine-Tuning Method',\n",
        "    fontsize=12,\n",
        "    title_fontsize=13,\n",
        "    bbox_to_anchor=(0.86, 0.5)\n",
        ")\n",
        "\n",
        "# Add a note about TER at the bottom\n",
        "fig.text(0.45, 0.01, '*For TER (Translation Edit Rate), a lower score is better.', ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H1Q5taNz0zu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZATION: Training and Validation Progress ---\n",
        "\n",
        "print(\"--- Generating Training and Validation Curves ---\")\n",
        "\n",
        "# The trainer object holds the history of the training run\n",
        "# Extract the logs to a pandas DataFrame for easy plotting\n",
        "try:\n",
        "  log_history = trainer.state.log_history\n",
        "  df_logs = pd.DataFrame(log_history)\n",
        "\n",
        "  # Separate training and evaluation logs\n",
        "  df_train = df_logs[df_logs['loss'].notna()].copy()\n",
        "  df_eval = df_logs[df_logs['eval_loss'].notna()].copy()\n",
        "\n",
        "  # Create the plot with two y-axes\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "  fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "  # Plot Training and Validation Loss on the primary y-axis (ax1)\n",
        "  ax1.set_xlabel('Training Step', fontsize=12)\n",
        "  ax1.set_ylabel('Loss', color='C0', fontsize=14)\n",
        "  ax1.plot(df_train['step'], df_train['loss'], color='C0', marker='.', linestyle=':', label='Training Loss')\n",
        "  ax1.plot(df_eval['step'], df_eval['eval_loss'], color='C0', marker='o', linestyle='-', label='Validation Loss')\n",
        "  ax1.tick_params(axis='y', labelcolor='C0')\n",
        "  ax1.legend(loc='upper left')\n",
        "\n",
        "  # Create a second y-axis for the BLEU score\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylabel('Validation BLEU Score', color='C1', fontsize=14)\n",
        "  ax2.plot(df_eval['step'], df_eval['eval_bleu'], color='C1', marker='s', linestyle='--', label='Validation BLEU')\n",
        "  ax2.tick_params(axis='y', labelcolor='C1')\n",
        "  ax2.legend(loc='upper right')\n",
        "\n",
        "  plt.title('Model Training and Validation Progress', fontsize=18, pad=20)\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "except NameError:\n",
        "  print(\"\\n⛔️ ERROR: The 'trainer' object was not found.\")\n",
        "  print(\"Please ensure you have run the main Fine-Tuning cell successfully before running this visualization cell.\")\n",
        "except Exception as e:\n",
        "  print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "VHyceAUCvD4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VISUALIZATION: Score Distribution (KDE Plot) ---\n",
        "print(\"\\n--- Generating Score Distribution Visualizations ---\")\n",
        "\n",
        "# Helper function to generate predictions for this specific analysis\n",
        "def generate_predictions_for_viz(model, tokenizer, inputs, src_lang, tgt_lang, batch_size=8):\n",
        "  \"\"\"\n",
        "  Generates translations for a list of input sentences for visualization or analysis.\n",
        "\n",
        "  This function uses a translation pipeline to generate translations for a list of input sentences,\n",
        "  processing them in batches to manage memory. It is designed for visualization or analysis tasks,\n",
        "  such as inspecting model outputs. Progress is displayed using a `tqdm` progress bar.\n",
        "\n",
        "  Args:\n",
        "    model (object): The translation model (e.g., a Hugging Face model).\n",
        "    tokenizer (object): The tokenizer associated with the model.\n",
        "    inputs (list[str]): A list of input sentences to translate.\n",
        "    src_lang (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang (str): Target language code (e.g., 'deu_Latn').\n",
        "    batch_size (int, optional): Number of sentences to process per batch. Defaults to 8.\n",
        "\n",
        "  Returns:\n",
        "    list[str]: A list of translated sentences.\n",
        "  \"\"\"\n",
        "  translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "  predictions = []\n",
        "  for i in tqdm(range(0, len(inputs), batch_size), desc=f\"Translating for Viz\"):\n",
        "    batch = inputs[i : i + batch_size]\n",
        "    preds = translator(batch, max_length=1200)\n",
        "    predictions.extend([p['translation_text'] for p in preds])\n",
        "  return predictions\n",
        "\n",
        "# --- Generate predictions for both models and both directions ---\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# Baseline Predictions\n",
        "print(\"\\nLoading BASELINE model...\")\n",
        "base_model_viz = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "baseline_preds_od = generate_predictions_for_viz(base_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)], ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "baseline_preds_do = generate_predictions_for_viz(base_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)], GERMAN_LANG_CODE, ODIA_LANG_CODE)\n",
        "del base_model_viz\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Fine-Tuned Predictions\n",
        "print(\"\\nLoading FINE-TUNED model...\")\n",
        "finetuned_model_viz = AutoModelForSeq2SeqLM.from_pretrained(FINAL_LORA_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\")\n",
        "finetuned_preds_od = generate_predictions_for_viz(finetuned_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)], ODIA_LANG_CODE, GERMAN_LANG_CODE)\n",
        "finetuned_preds_do = generate_predictions_for_viz(finetuned_model_viz, tokenizer, [ex[INPUT_FIELD] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)], GERMAN_LANG_CODE, ODIA_LANG_CODE)\n",
        "del finetuned_model_viz\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- Calculate sentence-level BLEU scores ---\n",
        "sbleu = sacrebleu.BLEU(effective_order=True)\n",
        "\n",
        "# For Odia -> German\n",
        "refs_od = [[ex[TARGET_FIELD]] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_ORI_TO_DEU)]\n",
        "baseline_scores_od = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(baseline_preds_od, refs_od)]\n",
        "finetuned_scores_od = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(finetuned_preds_od, refs_od)]\n",
        "df_dist_od = pd.DataFrame({'Baseline': baseline_scores_od, 'Fine-Tuned': finetuned_scores_od})\n",
        "\n",
        "# For German -> Odia\n",
        "refs_do = [[ex[TARGET_FIELD]] for ex in split_datasets['test'] if ex[INPUT_FIELD].startswith(PREFIX_DEU_TO_ORI)]\n",
        "baseline_scores_do = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(baseline_preds_do, refs_do)]\n",
        "finetuned_scores_do = [sbleu.sentence_score(pred, ref).score for pred, ref in zip(finetuned_preds_do, refs_do)]\n",
        "df_dist_do = pd.DataFrame({'Baseline': baseline_scores_do, 'Fine-Tuned': finetuned_scores_do})\n",
        "\n",
        "# --- Create the plots ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig.suptitle('Distribution of Sentence-Level BLEU Scores by Translation Direction', fontsize=18, y=1.02)\n",
        "\n",
        "# Plot 1: Odia -> German\n",
        "sns.kdeplot(data=df_dist_od, x='Baseline', fill=True, label='Baseline', clip=(0, 100), lw=3, ax=axes[0], color='C0')\n",
        "sns.kdeplot(data=df_dist_od, x='Fine-Tuned', fill=True, label='Fine-Tuned', clip=(0, 100), lw=3, ax=axes[0], color='C1')\n",
        "axes[0].set_title('Odia → German', fontsize=14)\n",
        "axes[0].set_xlabel('Sentence BLEU Score', fontsize=12)\n",
        "axes[0].set_ylabel('Density', fontsize=12)\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: German -> Odia\n",
        "sns.kdeplot(data=df_dist_do, x='Baseline', fill=True, label='Baseline', clip=(0, 100), lw=3, ax=axes[1], color='C0')\n",
        "sns.kdeplot(data=df_dist_do, x='Fine-Tuned', fill=True, label='Fine-Tuned', clip=(0, 100), lw=3, ax=axes[1], color='C1')\n",
        "axes[1].set_title('German → Odia', fontsize=14)\n",
        "axes[1].set_xlabel('Sentence BLEU Score', fontsize=12)\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z6ZmZEVH024p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWzlB8KxJXPc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}