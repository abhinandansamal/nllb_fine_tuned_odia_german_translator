{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq1_3dtzhzJp"
      },
      "source": [
        "# Analysis of Next-Token Prediction Distributions\n",
        "## Objective\n",
        "The objective of this notebook is to conduct a specific, in-depth qualitative analysis of the trained models' behavior. It focuses on visualizing how the **Baseline**, **Fully Fine-Tuned**, and **Adapter-Tuned (LoRA)** models choose an output from all possible options at a single, critical step in the translation process for both `Odia → German` and `German → Odia` directions. This serves to probe and compare the internal confidence and decision-making of each model.\n",
        "\n",
        "## Methodology\n",
        "The notebook implements the \"Next-Token Probability\" analysis. For a given source sentence and a partial translation (context), the script:\n",
        "\n",
        "1. Loads all three pre-trained and fine-tuned models.\n",
        "2. Uses a helper function, `get_next_token_distribution`, to perform a forward pass with each model to get the raw output logits for the next token.\n",
        "3. Converts these logits into a full probability distribution and identifies the top 10 most likely candidates.\n",
        "4. Uses a second helper function, `plot_topk_for_models`, to generate an interactive bar chart using Plotly to visualize this distribution for each model.\n",
        "\n",
        "## Workflow\n",
        "1. Mounts Google Drive to access the saved model artifacts.\n",
        "2. Loads all three models (Baseline, Full FT, and LoRA) and the NLLB tokenizer into memory.\n",
        "3. Defines the helper functions for getting the token distribution and for plotting.\n",
        "4. Executes the analysis by calling the plotting function for each of the three models, once for the `Odia → German` test case and once for the `German → Odia` test case.\n",
        "5. Displays the resulting interactive plots in the notebook's output.\n",
        "\n",
        "## Input & Output\n",
        "* **Input:** The saved model artifacts for the Fully Fine-Tuned and LoRA models, located in Google Drive.\n",
        "* **Output:** A series of six interactive Plotly bar charts printed to the notebook console (one for each model and each translation direction), providing a detailed comparative analysis of the models' next-token prediction confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C62VFjUhxRY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNKHaBJdh6D1"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets sacrebleu torch accelerate pandas bitsandbytes peft seaborn plotly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- All Installed Packages (pip list) ---\")\n",
        "!pip list"
      ],
      "metadata": {
        "id": "5CkNH_l4WrIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh8LHdAUh8HT"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "from IPython.display import display\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70g9clKJiA65"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBizaESah-xo"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
        "FFT_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/nllb-odia-german-translator_model_final\"\n",
        "LORA_MODEL_PATH = \"/content/drive/MyDrive/Thesis/model/lora-odia-german-translator\"\n",
        "\n",
        "\n",
        "# Language codes are still needed for the tokenizer\n",
        "ODIA_LANG_CODE = \"ory_Orya\"\n",
        "GERMAN_LANG_CODE = \"deu_Latn\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load All Models and Tokenizer ---\n",
        "print(\"--- Loading all models for analysis ---\")\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, src_lang=ODIA_LANG_CODE, tgt_lang=GERMAN_LANG_CODE)\n",
        "\n",
        "# Load Baseline Model\n",
        "print(\"Loading Baseline model...\")\n",
        "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "# Load Full Fine-Tuned Model\n",
        "print(\"Loading Fully Fine-Tuned model...\")\n",
        "fft_model = AutoModelForSeq2SeqLM.from_pretrained(FFT_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "# Load Adapter-Tuned (LoRA) Model\n",
        "print(\"Loading Adapter-Tuned (LoRA) model...\")\n",
        "lora_base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "lora_model = PeftModel.from_pretrained(lora_base_model, LORA_MODEL_PATH)\n",
        "lora_model.eval()\n",
        "\n",
        "print(\"\\n✅ All models loaded successfully.\")"
      ],
      "metadata": {
        "id": "svnLg2ODSKLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next-Token Probability Distribution"
      ],
      "metadata": {
        "id": "bKKN4UNkwayu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentences and partial translations to test\n",
        "test_sentence_german = \"translate German to Odia: Die Feuerwehr musste zahlreiche Menschen mit Booten in Sicherheit bringen.\"\n",
        "partial_odia_context = \"ଅଗ୍ନିଶମ ବାହିନୀକୁ\"\n",
        "\n",
        "test_sentence_odia = \"translate Odia to German: ମନ୍ତ୍ରୀ ଘୋଷଣା କଲେ ଯେ ଏହି ନୂଆ ରାଜପଥ ଆସନ୍ତା ବର୍ଷ ସୁଦ୍ଧା ସମ୍ପୂର୍ଣ୍ଣ ହେବ।\"\n",
        "partial_german_context = \"Der Minister kündigte an, dass\""
      ],
      "metadata": {
        "id": "ERF7HcKGyBN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of your loaded models for easy iteration\n",
        "models_to_analyze = {\n",
        "    \"Baseline\": baseline_model,\n",
        "    \"Fully Fine-Tuned\": fft_model,\n",
        "    \"Adapter-Tuned (LoRA)\": lora_model\n",
        "}"
      ],
      "metadata": {
        "id": "orVGlCDiyFZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function: returns DataFrame of top-k next-token probabilities\n",
        "def get_next_token_distribution(model, tokenizer, src_prompt, tgt_prompt, src_lang_code, tgt_lang_code,\n",
        "                                top_k=10, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  \"\"\"\n",
        "  Computes the top-k next-token probability distribution for a translation model.\n",
        "\n",
        "  This function tokenizes a source and target prompt, performs a forward pass through the model\n",
        "  to obtain logits for the next token, and returns a DataFrame containing the top-k tokens,\n",
        "  their IDs, and their probabilities. It handles token decoding to display readable labels,\n",
        "  falling back to token names for undecodable tokens.\n",
        "\n",
        "  Args:\n",
        "    model (object): The translation model (e.g., a Hugging Face sequence-to-sequence model).\n",
        "    tokenizer (object): The tokenizer associated with the model, supporting `convert_ids_to_tokens` and `decode` methods.\n",
        "    src_prompt (str): The source text prompt for translation.\n",
        "    tgt_prompt (str): The partial or complete target text prompt for next-token prediction.\n",
        "    src_lang_code (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang_code (str): Target language code (e.g., 'deu_Latn').\n",
        "    top_k (int, optional): Number of top tokens to return. Defaults to 10.\n",
        "    device (str, optional): Device for computation ('cuda' or 'cpu'). Defaults to 'cuda' if available, else 'cpu'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: A DataFrame with columns:\n",
        "    - 'token_str': Readable token strings (decoded or token names with subword notation).\n",
        "    - 'token_id': Token IDs from the tokenizer's vocabulary.\n",
        "    - 'probability': Probabilities of the top-k tokens.\n",
        "  \"\"\"\n",
        "  # Tokenize the source prompt\n",
        "  inputs = tokenizer(src_prompt, return_tensors='pt').to(device)\n",
        "  # Tokenize the (possibly partial) target prompt, as decoder_input_ids\n",
        "  tgt_tokens = tokenizer(tgt_prompt, return_tensors='pt', add_special_tokens=False)['input_ids'].to(device)\n",
        "  decoder_input_ids = tgt_tokens\n",
        "\n",
        "  # Forward pass to get logits for next-token prediction\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], decoder_input_ids=decoder_input_ids)\n",
        "    next_token_logits = outputs.logits[0, -1]  # (vocab_size,)\n",
        "\n",
        "  probs = F.softmax(next_token_logits, dim=-1)\n",
        "  topk_probs, topk_indices = torch.topk(probs, top_k)\n",
        "\n",
        "  # Always show a readable label: decode and fallback to token name\n",
        "  topk_tokens = [tokenizer.convert_ids_to_tokens([idx.item()])[0] for idx in topk_indices]\n",
        "  topk_decoded = [tokenizer.decode([idx]).strip() for idx in topk_indices]\n",
        "  topk_display = [\n",
        "      f\"{d if d else '[UNK]'} [{t}]\" if d else f\"[{t}]\"\n",
        "      for t, d in zip(topk_tokens, topk_decoded)\n",
        "  ]\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'token_str': topk_display,\n",
        "      'token_id': topk_indices.cpu().numpy(),\n",
        "      'probability': topk_probs.cpu().numpy()\n",
        "  })\n",
        "  return df"
      ],
      "metadata": {
        "id": "tM3Wgcmrxyzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to visualize for all models in a row\n",
        "def plot_topk_for_models(models_dict, tokenizer, src_prompt, tgt_prompt, src_lang_code, tgt_lang_code, top_k=10):\n",
        "  \"\"\"\n",
        "  Visualizes top-k next-token probability distributions for multiple models.\n",
        "\n",
        "  This function generates bar plots for each model in the provided dictionary, showing the\n",
        "  top-k token probabilities for a given source and target prompt pair. It uses the\n",
        "  `get_next_token_distribution` function to compute probabilities and creates interactive\n",
        "  bar plots with Plotly Express, displaying token strings and their probabilities.\n",
        "\n",
        "  Args:\n",
        "    models_dict (dict[str, object]): A dictionary mapping model names to translation models (e.g., Hugging Face sequence-to-sequence models).\n",
        "    tokenizer (object): The tokenizer shared by all models, supporting `convert_ids_to_tokens` and `decode` methods.\n",
        "    src_prompt (str): The source text prompt for translation.\n",
        "    tgt_prompt (str): The partial or complete target text prompt for next-token prediction.\n",
        "    src_lang_code (str): Source language code (e.g., 'ory_Orya').\n",
        "    tgt_lang_code (str): Target language code (e.g., 'deu_Latn').\n",
        "    top_k (int, optional): Number of top tokens to display in each plot. Defaults to 10.\n",
        "\n",
        "  Returns:\n",
        "    list[tuple[str, object]]: A list of tuples, each containing the model name and its corresponding Plotly Express figure object (bar plot).\n",
        "  \"\"\"\n",
        "  figs = []\n",
        "  for model_name, model in models_dict.items():\n",
        "    df = get_next_token_distribution(\n",
        "        model, tokenizer, src_prompt, tgt_prompt,\n",
        "        src_lang_code, tgt_lang_code, top_k=top_k\n",
        "    )\n",
        "    fig = px.bar(\n",
        "        df, x='token_str', y='probability',\n",
        "        title=f\"{model_name} - Next Token Distribution\",\n",
        "        labels={\"token_str\": \"Token [subword]\", \"probability\": \"Probability\"},\n",
        "        text=\"probability\"\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        texttemplate='%{text:.3f}', textposition='outside', marker_color=\"royalblue\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        yaxis=dict(range=[0, 1]), xaxis={'categoryorder':'total descending'},\n",
        "        bargap=0.2, xaxis_tickangle=-30\n",
        "    )\n",
        "\n",
        "    figs.append((model_name, fig))\n",
        "\n",
        "  return figs"
      ],
      "metadata": {
        "id": "qZmuG4WByJ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next-Token Probability Distribution: Odia → German\n",
        "# Example: Sampling next token for <Odia, partial German>\n",
        "src_text = test_sentence_odia  # \"translate Odia to German: ...\"\n",
        "partial = partial_german_context  # E.g. partial output \"Der Minister kündigte an, dass\""
      ],
      "metadata": {
        "id": "TmODQW1942xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figs = plot_topk_for_models(\n",
        "    models_to_analyze, tokenizer,\n",
        "    src_prompt=src_text, tgt_prompt=partial,\n",
        "    src_lang_code=ODIA_LANG_CODE, tgt_lang_code=GERMAN_LANG_CODE,\n",
        "    top_k=10\n",
        ")"
      ],
      "metadata": {
        "id": "PPaZ1dqdyLJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all plots\n",
        "for model_name, fig in figs:\n",
        "  print(model_name)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "YSxkQASNyK5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next-Token Probability Distribution: German → Odia\n",
        "# Source: German prompt (as input to the model)\n",
        "src_text = test_sentence_german         # e.g., \"translate German to Odia: Die Feuerwehr musste zahlreiche Menschen mit Booten in Sicherheit bringen.\"\n",
        "# Partial output: Odia context to prompt the model for predicting the next Odia token\n",
        "partial = partial_odia_context          # e.g., \"ଅଗ୍ନିଶମ ବାହିନୀକୁ\""
      ],
      "metadata": {
        "id": "bsfJTQD76NjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figs = plot_topk_for_models(\n",
        "    models_to_analyze, tokenizer,\n",
        "    src_prompt=src_text, tgt_prompt=partial,\n",
        "    src_lang_code=GERMAN_LANG_CODE, tgt_lang_code=ODIA_LANG_CODE,\n",
        "    top_k=10\n",
        ")"
      ],
      "metadata": {
        "id": "lTR2BVgt6NhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, fig in figs:\n",
        "  print(model_name)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "dCVz_rpC6NeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up memory\n",
        "print(\"\\nCleaning up models from memory...\")\n",
        "del baseline_model, fft_model, lora_base_model, lora_model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✅ Analysis complete.\")"
      ],
      "metadata": {
        "id": "f2CGVUPcxmLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}