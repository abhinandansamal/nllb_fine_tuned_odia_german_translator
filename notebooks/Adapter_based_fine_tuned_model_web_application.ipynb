{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Web Application for Adapter-Tuned (LoRA) Model\n",
        "## Objective\n",
        "The objective of this notebook is to test and launch a live, interactive web application for the Adapter-Tuned (LoRA) NLLB translation model. It contains the complete, self-contained Python code required to build a Gradio-based user interface that allows users to perform bidirectional Odia-German translation.\n",
        "\n",
        "## Methodology\n",
        "The script is designed to be a complete web application that showcases the unique loading process for a PEFT model.\n",
        "\n",
        "1. **Model Loading:** It first loads the original, large **base NLLB model** in 8-bit precision. It then loads the small, fine-tuned **LoRA adapters** from their repository on the Hugging Face Hub and applies them to the base model.\n",
        "2. **Language Detection:** It implements a robust, hybrid language detection system, prioritizing a script-based check for Odia and using the `langdetect` library as a fallback.\n",
        "3. **Translation Logic:** It defines a central `translate_text` function that takes user input, runs the detection logic, and calls the loaded model to generate the translation.\n",
        "4. **Web Interface:** It uses the `gradio` library to create a clean user interface, complete with text boxes, a dropdown for manual language selection, and example sentences.\n",
        "\n",
        "## Workflow\n",
        "1. Installs all required libraries (`gradio`, `transformers`, `peft`, etc.).\n",
        "2. Loads the base NLLB model and the LoRA adapters from the Hub.\n",
        "3. Defines the language detection and translation functions.\n",
        "4. Creates the Gradio `Interface` object.\n",
        "5. Launches the web application, creating a temporary public URL for testing in the Colab environment.\n",
        "\n",
        "## Input & Output\n",
        "* **Input:** Text entered by a user into the Gradio web interface.\n",
        "* **Output:** A live, interactive Gradio web application for bidirectional Odia-German translation powered by the LoRA fine-tuned model."
      ],
      "metadata": {
        "id": "meEK-iQMYRrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall all potentially conflicting packages\n",
        "!pip uninstall -y torch torchvision torchaudio transformers gradio langdetect sentencepiece accelerate huggingface-hub safetensors peft bitsandbytes torchtune sentence-transformers timm"
      ],
      "metadata": {
        "id": "95HSVEXNRBZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear pip cache\n",
        "!pip cache purge"
      ],
      "metadata": {
        "id": "QrBX-N-zSJ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install transformers==4.52.4\n",
        "!pip install gradio==5.31.0\n",
        "!pip install langdetect==1.0.9\n",
        "!pip install sentencepiece==0.2.0\n",
        "!pip install huggingface-hub==0.33.0\n",
        "!pip install accelerate==1.7.0\n",
        "!pip install safetensors==0.5.3\n",
        "!pip install peft==0.15.2\n",
        "!pip install bitsandbytes==0.46.1"
      ],
      "metadata": {
        "id": "sepLNeGWSMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installations\n",
        "!pip show torch transformers gradio langdetect sentencepiece huggingface-hub accelerate safetensors bitsandbytes"
      ],
      "metadata": {
        "id": "L-4AnQfbcccY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check CUDA availability and version\n",
        "import torch\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "0QPhP9IpSoCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the Runtime. Then execute the code below."
      ],
      "metadata": {
        "id": "zr-EkKt4aSEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear cache\n",
        "!rm -rf ~/.cache/huggingface"
      ],
      "metadata": {
        "id": "uKSDn3GEaPQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "7f9hYbq3dBVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from langdetect import detect, LangDetectException\n",
        "import torch\n",
        "import traceback\n",
        "import logging\n",
        "import re"
      ],
      "metadata": {
        "id": "x8-Oa2_WxxkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "oKkw5ekMn9wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load Adapter-based fine-tuned model (from the hub)\n",
        "BASE_MODEL_HUB_ID = \"facebook/nllb-200-distilled-600M\"\n",
        "ADAPTER_MODEL_HUB_ID = \"abhinandansamal/nllb-200-distilled-600M-LoRA-finetuned-odia-german-bidirectional\"\n",
        "ODIA_LANG_CODE = \"ory_Orya\"\n",
        "GERMAN_LANG_CODE = \"deu_Latn\""
      ],
      "metadata": {
        "id": "V59QD9TzyVT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading the Adapter-based fine-tuned bidirectional model...\")\n",
        "translator = None\n",
        "try:\n",
        "  # Step A: Load the original base model (quantized for efficiency)\n",
        "  print(f\"Loading base model: {BASE_MODEL_HUB_ID}...\")\n",
        "  bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "  base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "      BASE_MODEL_HUB_ID,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  # Load the NLLB tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_HUB_ID, src_lang=ODIA_LANG_CODE, tgt_lang=GERMAN_LANG_CODE)\n",
        "\n",
        "  # Step B: Load the LoRA adapters and apply them to the base model\n",
        "  print(f\"Loading and applying adapters from: {ADAPTER_MODEL_HUB_ID}...\")\n",
        "  model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_HUB_ID)\n",
        "  model.eval() # Set the model to evaluation mode\n",
        "\n",
        "  # Step C: Create the pipeline with the complete, merged model\n",
        "  print(\"Creating translation pipeline...\")\n",
        "  translator = pipeline(\n",
        "      \"translation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      src_lang=ODIA_LANG_CODE, # Provide defaults for the pipeline\n",
        "      tgt_lang=GERMAN_LANG_CODE\n",
        "  )\n",
        "  print(\"✅ Model and pipeline loaded successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"❌ Detailed error: {traceback.format_exc()}\")\n",
        "  translator = None"
      ],
      "metadata": {
        "id": "CpDQLDlTy2Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script-based Odia detection\n",
        "def is_odia_text(text):\n",
        "  \"\"\"\n",
        "  Checks if the input text contains Odia script characters.\n",
        "\n",
        "  This function uses a regular expression to detect the presence of characters in the Unicode\n",
        "  range for Odia script (U+0B00–U+0B7F). It logs the detection result and returns True if Odia\n",
        "  characters are found, False otherwise. Empty or whitespace-only inputs are considered invalid.\n",
        "\n",
        "  Args:\n",
        "    text (str): The input text to check for Odia script characters.\n",
        "\n",
        "  Returns:\n",
        "    bool: True if the text contains at least one Odia script character, False otherwise.\n",
        "\n",
        "  Example:\n",
        "    >>> import logging\n",
        "    >>> logging.basicConfig(level=logging.INFO)\n",
        "    >>> logger = logging.getLogger()\n",
        "    >>> is_odia_text(\"ନମସ୍କାର\")\n",
        "    INFO:root:Odia script detection for 'ନମସ୍କାର...': True\n",
        "    True\n",
        "    >>> is_odia_text(\"Hallo\")\n",
        "    INFO:root:Odia script detection for 'Hallo...': False\n",
        "    False\n",
        "    >>> is_odia_text(\"\")\n",
        "    False\n",
        "  \"\"\"\n",
        "  if not text or not text.strip():\n",
        "    return False\n",
        "\n",
        "  odia_pattern = re.compile(r'[\\u0B00-\\u0B7F]')\n",
        "\n",
        "  match = odia_pattern.search(text)\n",
        "\n",
        "  logger.info(f\"Odia script detection for '{text[:50]}...': {bool(match)}\")\n",
        "\n",
        "  return bool(match)"
      ],
      "metadata": {
        "id": "_BbtGLRDnfW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation logic\n",
        "def translate_text(input_text, source_lang=\"auto\"):\n",
        "  \"\"\"\n",
        "  Translates text between Odia and German, with automatic or manual language detection.\n",
        "\n",
        "  This function translates input text using a preloaded translation model, supporting Odia-to-German\n",
        "  and German-to-Odia directions. If `source_lang` is 'auto', it prioritizes script-based detection\n",
        "  for Odia using `is_odia_text`, falling back to `langdetect` for non-Odia text. It logs the process\n",
        "  and handles errors gracefully, returning error messages for invalid inputs or failures.\n",
        "\n",
        "  Args:\n",
        "    input_text (str): The text to translate.\n",
        "    source_lang (str, optional): Source language code ('auto', 'or' for Odia, 'de' for German).\n",
        "                                 Defaults to 'auto' for automatic detection.\n",
        "\n",
        "  Returns:\n",
        "    str: The translated text or an error message if translation or language detection fails.\n",
        "  \"\"\"\n",
        "  if translator is None:\n",
        "    return \"Error: Model could not be loaded.\"\n",
        "  if not input_text or not input_text.strip():\n",
        "    return \"Error: Input text is empty.\"\n",
        "\n",
        "  try:\n",
        "    if source_lang != \"auto\":\n",
        "      detected_lang = source_lang\n",
        "      logger.info(f\"Manual source language selected: {detected_lang}\")\n",
        "    else:\n",
        "      # Prioritize script detection for Odia\n",
        "      if is_odia_text(input_text):\n",
        "        detected_lang = \"or\"\n",
        "        logger.info(f\"Detected language (script-based): {detected_lang}\")\n",
        "      else:\n",
        "        # Use langdetect for non-Odia text\n",
        "        try:\n",
        "          detected_lang = detect(input_text)\n",
        "          logger.info(f\"Detected language (langdetect): {detected_lang}\")\n",
        "        except LangDetectException as e:\n",
        "          logger.error(f\"LangDetectException: {e}\")\n",
        "          return \"Error: Could not detect language. Please select Odia or German manually.\"\n",
        "        except Exception as e:\n",
        "          logger.error(f\"Unexpected error in langdetect: {e}\")\n",
        "          return \"Error: Language detection failed. Please select Odia or German manually.\"\n",
        "\n",
        "    if detected_lang == \"or\":\n",
        "      result = translator(input_text, src_lang=ODIA_LANG_CODE, tgt_lang=GERMAN_LANG_CODE, max_length=1024)\n",
        "      logger.info(f\"Translating Odia to German: {input_text[:50]}...\")\n",
        "    elif detected_lang == \"de\":\n",
        "      result = translator(input_text, src_lang=GERMAN_LANG_CODE, tgt_lang=ODIA_LANG_CODE, max_length=1024)\n",
        "      logger.info(f\"Translating German to Odia: {input_text[:50]}...\")\n",
        "    else:\n",
        "      logger.warning(f\"Unsupported language detected: {detected_lang}\")\n",
        "      return f\"Error: Translation from '{detected_lang}' is not supported. Please select Odia or German.\"\n",
        "\n",
        "    return result[0][\"translation_text\"]\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Translation error: {traceback.format_exc()}\")\n",
        "    return f\"Error: Translation failed - {str(e)}\""
      ],
      "metadata": {
        "id": "UHQhh5zhzyTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test translation\n",
        "if translator is None:\n",
        "  print(\"Translator not initialized due to previous errors.\")\n",
        "else:\n",
        "  odia_text = \"କମ୍ପ୍ୟୁଟର ଆଧାରିତ ଏହି ପରୀକ୍ଷାର ଫଳାଫଳ ୧୫ ଜୁଲାଇରେ ଘୋଷଣା ହେବାର ଆଶା କରାଯାଉଛି।\"\n",
        "  try:\n",
        "    translation = translate_text(odia_text, source_lang=\"auto\")\n",
        "    print(f\"Odia: {odia_text}\")\n",
        "    print(f\"German: {translation}\")\n",
        "  except Exception as e:\n",
        "    print(f\"❌ Error during translation: {e}\")"
      ],
      "metadata": {
        "id": "tDobO9kt8msq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "title = \"LoRA Bidirectional Odia ↔ German Translator\"\n",
        "description = \"\"\"\n",
        "An Adapter-Based (LoRA) Fine-tuned NLLB model for translating between Odia and German.\n",
        "Enter text and select the source language (or use auto-detection).\n",
        "\"\"\"\n",
        "examples = [\n",
        "    [\"ଆଜି ପାଗ ବହୁତ ଭଲ ଅଛି।\", \"or\"],  # \"The weather is very nice today.\"\n",
        "    [\"Die derzeitige Wachstumsrate von 6,5 Prozent ist sehr lobenswert.\", \"de\"]  # \"The current growth rate of 6.5 percent is very commendable.\"\n",
        "]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=translate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Input Text (Odia or German)\", placeholder=\"Type or paste text here...\"),\n",
        "        gr.Dropdown(choices=[\"auto\", \"or\", \"de\"], label=\"Source Language\", value=\"auto\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(lines=5, label=\"Translation\"),\n",
        "    title=title,\n",
        "    description=description,\n",
        "    examples=examples,\n",
        "    allow_flagging=\"never\",\n",
        "    theme=gr.themes.Soft()\n",
        ")"
      ],
      "metadata": {
        "id": "R0bnmVpYkWm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch in Colab\n",
        "print(\"\\nLaunching Gradio interface...\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "gcfHArZBlBia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvKZkNdxlDcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}