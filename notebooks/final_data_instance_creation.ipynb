{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Parallel Corpus to Structured JSONL Conversion\n","## Objective\n","The objective of this notebook is to perform a critical data transformation step in the research pipeline. It takes two separate, line-aligned plain text files—one containing the Odia corpus and one containing the German corpus—and merges them into a single, structured JSON Lines (`.jsonl`) file. This process enriches the raw parallel data with a consistent structure and placeholder metadata fields, preparing it for subsequent manual annotation and the creation of a bidirectional training corpus.\n","\n","## Methodology\n","The script automates the creation of a structured parallel corpus by:\n","\n","* **Loading Parallel Data:** Reading the Odia and German text files into memory.\n","\n","* **Alignment Verification:** Performing a crucial validation step to ensure both input files have the exact same number of lines, preventing data misalignment errors downstream.\n","\n","* **Structured Transformation:** Iterating through the parallel lines and creating a distinct JSON object for each pair. Each object is assigned a unique ID and includes fields for the Odia sentence (`sentence_ory_Orya`), the German sentence (`sentence_deu_Latn`), and placeholders for metadata (`URL`, `domain`, `topic`, `publication_date`).\n","\n","* **JSONL Output:** Writing each JSON object as a new line to the output file, adhering to the JSON Lines standard.\n","\n","## Workflow\n","The notebook executes the following sequential steps:\n","\n","1. Mounts Google Drive to access the source corpora and define a persistent save location.\n","\n","2. Configures the input and output file paths and defines the placeholder metadata.\n","\n","3. Reads the content of the source Odia and German `.txt` files.\n","\n","4. Validates that the line counts of both files match.\n","\n","5. Generates the structured `.jsonl` file by merging the parallel data and adding the defined structure.\n","\n","6. Saves the final `.jsonl` corpus to the specified Google Drive directory.\n","\n","## Input & Output\n","* **Input:** Two `.txt` files, each containing one sentence (or text segment) per line, where line `N` of the Odia file is the direct translation of line `N` of the German file.\n","* **Output:** A single `.jsonl` file (e.g., `authentic_complete_corpus.jsonl`), where each line is a complete JSON object ready for manual metadata annotation."],"metadata":{"id":"om8XHo7cpVgl"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1EFOHpHz7TzD","executionInfo":{"status":"ok","timestamp":1751439590317,"user_tz":-120,"elapsed":23072,"user":{"displayName":"Abhinandan Samal","userId":"05606082246540318546"}},"outputId":"551da459-1df0-472c-82b1-40b1582e66a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww5p7GvbOlBT"},"outputs":[],"source":["import json\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","source":["# --- 1. CONFIGURATION ---\n","\n","# --- Input Files ---\n","# Make sure these files have the exact same number of lines!\n","ODIA_CORPUS_FILE = '/content/drive/MyDrive/Thesis/data/raw/authentic_odia_corpus_v1.txt'\n","GERMAN_CORPUS_FILE = '/content/drive/MyDrive/Thesis/data/raw/authentic_german_corpus_v1.txt'\n","\n","# --- Output File ---\n","OUTPUT_JSONL_FILE = '/content/drive/MyDrive/Thesis/data/transformed/authentic_corpus.jsonl'"],"metadata":{"id":"u6phuuUFUCRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Placeholder values for manual insertion ---\n","PLACEHOLDER_URL = \"MANUAL_INSERT_URL\"\n","PLACEHOLDER_DOMAIN = \"MANUAL_INSERT_DOMAIN\" # e.g., \"Dharitri\" or \"Sambad\"\n","PLACEHOLDER_TOPIC = \"MANUAL_INSERT_TOPIC\"   # e.g., \"national-news\"\n","PLACEHOLDER_DATE = \"YYYY\""],"metadata":{"id":"xs_vjWBbUKtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_structured_corpus():\n","  \"\"\"\n","  Creates a structured JSON Lines (.jsonl) file from parallel Odia and German text files.\n","\n","  This function reads two input files (specified by `ODIA_CORPUS_FILE` and `GERMAN_CORPUS_FILE`),\n","  verifies they have the same number of lines, filters out empty lines, and generates a JSON Lines\n","  file containing paired Odia and German sentences. Each valid entry is a JSON object with a unique\n","  ID, placeholder metadata (URL, domain, topic, publication date), and the corresponding sentences.\n","  The output is written to `OUTPUT_JSONL_FILE`. Progress is displayed using a `tqdm` progress bar.\n","\n","  Note:\n","    - Assumes global variables `ODIA_CORPUS_FILE`, `GERMAN_CORPUS_FILE`, `OUTPUT_JSONL_FILE`,\n","      `PLACEHOLDER_URL`, `PLACEHOLDER_DOMAIN`, `PLACEHOLDER_TOPIC`, and `PLACEHOLDER_DATE` are defined.\n","    - Skips blank lines in either file to ensure only valid sentence pairs are included.\n","    - Uses UTF-8 encoding for reading and writing files to handle multilingual text.\n","    - Requires the `tqdm` and `json` libraries.\n","  \"\"\"\n","  if not os.path.exists(ODIA_CORPUS_FILE) or not os.path.exists(GERMAN_CORPUS_FILE):\n","    print(f\"⛔️ ERROR: One or both input files not found.\")\n","    return\n","\n","  print(\"Reading source files...\")\n","  with open(ODIA_CORPUS_FILE, 'r', encoding='utf-8') as f_ori:\n","    odia_lines = [line.strip() for line in f_ori]\n","  with open(GERMAN_CORPUS_FILE, 'r', encoding='utf-8') as f_deu:\n","    german_lines = [line.strip() for line in f_deu]\n","\n","  if len(odia_lines) != len(german_lines):\n","    print(\"⛔️ FATAL ERROR: The number of lines in the source files do not match!\")\n","    return\n","\n","  total_lines = len(odia_lines)\n","  print(f\"✅ Files are aligned. Found {total_lines} total lines (including blanks) to process.\")\n","\n","  # --- Step 4: Create the JSONL file, SKIPPING blank lines ---\n","  print(f\"Generating structured data in '{OUTPUT_JSONL_FILE}'...\")\n","\n","  # We will use a manual counter for the ID to keep it sequential for valid entries\n","  entry_id = 1\n","\n","  with open(OUTPUT_JSONL_FILE, 'w', encoding='utf-8') as f_out:\n","    for odia_line, german_line in tqdm(zip(odia_lines, german_lines), total=total_lines, desc=\"Processing Lines\"):\n","      # Only proceed if BOTH lines contain text after stripping whitespace.\n","      if odia_line and german_line:\n","        # Create the Python dictionary for this valid instance\n","        data_instance = {\n","            'id': entry_id,\n","            'URL': PLACEHOLDER_URL,\n","            'domain': PLACEHOLDER_DOMAIN,\n","            'topic': PLACEHOLDER_TOPIC,\n","            'publication_date': PLACEHOLDER_DATE,\n","            'sentence_ory_Orya': odia_line,\n","            'sentence_deu_Latn': german_line\n","        }\n","\n","        # Write the JSON object to the file\n","        f_out.write(json.dumps(data_instance, ensure_ascii=False) + '\\n')\n","\n","        # Increment the ID only for valid entries\n","        entry_id += 1\n","\n","  print(f\"\\n✅ Success! Your structured corpus has been created with {entry_id - 1} valid entries.\")\n","  print(\"Blank lines have been ignored.\")\n","\n","if __name__ == \"__main__\":\n","  create_structured_corpus()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azJxecJfUhGr","executionInfo":{"status":"ok","timestamp":1751439608803,"user_tz":-120,"elapsed":9162,"user":{"displayName":"Abhinandan Samal","userId":"05606082246540318546"}},"outputId":"e6e4d583-792a-4e4d-81bc-bb9d49a5600b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading source files...\n","✅ Files are aligned. Found 7351 total lines (including blanks) to process.\n","Generating structured data in '/content/drive/MyDrive/Thesis/data/transformed/authentic_corpus.jsonl'...\n"]},{"output_type":"stream","name":"stderr","text":["Processing Lines: 100%|██████████| 7351/7351 [00:00<00:00, 114426.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Success! Your structured corpus has been created with 3676 valid entries.\n","Blank lines have been ignored.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_fsvJlrk7yEx"},"execution_count":null,"outputs":[]}]}