{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPS9rwzSg8nA1w1Zt23QBnC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bidirectional Corpus Transformation\n","## Objective\n","The objective of this notebook is to transform a standard, unidirectional parallel corpus into a **bidirectional corpus** suitable for multitask fine-tuning. It takes a structured `.jsonl` file containing aligned Odia-German sentence pairs and creates a new, larger dataset where each original pair is represented as two distinct training instances: one for the `Odia → German` direction and one for the `German → Odia` direction.\n","\n","## Methodology\n","The script programmatically restructures the dataset for a multitask learning setup by:\n","\n","* **Reading Structured Data:** Loading the parallel corpus from the source `.jsonl` file, which includes rich metadata for each sentence pair.\n","\n","* **Instance Duplication and Prefixing:** For each parallel pair, it generates two new JSON objects. Each new object contains a new `input_text` field, which is created by prepending a task-specific prefix (e.g., `\"translate Odia to German:\"`) to the appropriate source sentence.\n","\n","* **Shuffling:** The final, doubled-sized list of training instances is thoroughly shuffled to ensure that the model does not see translation directions in a predictable order during training.\n","\n","* **Saving the Final Corpus:** The script saves the shuffled, bidirectional data to a new `.jsonl` file, which will serve as the master corpus for splitting into training, validation, and test sets.\n","\n","## Workflow\n","1. Mounts Google Drive to access the source `.jsonl` file and define a persistent save location.\n","\n","2. Configures the input and output file paths and defines the task prefixes.\n","\n","3. Reads the content of the source `final_corpus_poc.jsonl` file.\n","\n","4. Iterates through each record, creating the two corresponding bidirectional instances while preserving all original metadata.\n","\n","5. Shuffles the newly created list of 7,352 instances.\n","\n","6. Saves the final dataset to the `bidirectional_corpus.jsonl` file.\n","\n","## Input & Output\n","* **Input:** A single `.jsonl` file (`final_corpus_poc.jsonl`) containing structured, parallel Odia-German data.\n","* **Output:** A single, larger `.jsonl` file (`bidirectional_corpus.jsonl`) containing twice the number of entries, formatted for bidirectional model training."],"metadata":{"id":"omkD9_DcsI99"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t8QpMt6Yr6K","executionInfo":{"status":"ok","timestamp":1751439249478,"user_tz":-120,"elapsed":20597,"user":{"displayName":"Abhinandan Samal","userId":"05606082246540318546"}},"outputId":"def5e3e3-aed0-400e-e14e-4c72e2ede009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import json\n","import random\n","import os\n","from tqdm import tqdm"],"metadata":{"id":"u9iRJI11Yp3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AB2AEwR3YnVj"},"outputs":[],"source":["# --- CONFIGURATION ---\n","INPUT_FILE = \"/content/drive/MyDrive/Thesis/data/transformed/authentic_corpus_final.jsonl\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Thesis/data/transformed/bidirectional_corpus_final.jsonl\"\n","\n","# Define the exact prefixes the model will learn\n","PREFIX_ORI_TO_DEU = \"translate Odia to German: \"\n","PREFIX_DEU_TO_ORI = \"translate German to Odia: \"\n","\n","# Field names from your JSONL file\n","SOURCE_SENTENCE_FIELD = \"sentence_ory_Orya\"\n","TARGET_SENTENCE_FIELD = \"sentence_deu_Latn\""]},{"cell_type":"code","source":["def create_rich_bidirectional_dataset():\n","  \"\"\"\n","  Creates a bidirectional JSON Lines (.jsonl) dataset from a structured parallel corpus.\n","\n","  This function reads a JSON Lines file containing parallel Odia and German sentences, generates\n","  bidirectional training instances (Odia-to-German and German-to-Odia), and preserves metadata\n","  (e.g., URL, domain, topic, publication date). Each input record is transformed into two instances:\n","  one with the Odia sentence as input and German as target, and another with German as input and\n","  Odia as target. A prefix is added to input texts to indicate translation direction. The resulting\n","  dataset is shuffled and saved to a new JSON Lines file.\n","\n","  Note:\n","    - Assumes global variables `INPUT_FILE`, `OUTPUT_FILE`, `SOURCE_SENTENCE_FIELD`,\n","      `TARGET_SENTENCE_FIELD`, `PREFIX_ORI_TO_DEU`, and `PREFIX_DEU_TO_ORI` are defined.\n","    - Skips records with missing or non-string sentences.\n","    - Uses UTF-8 encoding for reading and writing files to handle multilingual text.\n","    - Requires the `json`, `random`, and `tqdm` libraries.\n","  \"\"\"\n","  if not os.path.exists(INPUT_FILE):\n","    print(f\"⛔️ ERROR: Input file '{INPUT_FILE}' not found.\")\n","    return\n","\n","  print(f\"Reading original structured corpus from '{INPUT_FILE}'...\")\n","  with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n","    original_data = [json.loads(line) for line in f]\n","\n","  bidirectional_data = []\n","  new_id_counter = 1\n","  print(\"Creating bidirectional training instances with metadata...\")\n","\n","  for record in tqdm(original_data, desc=\"Processing Records\"):\n","    # --- Extract all data from the original record ---\n","    original_id = record.get('id')\n","    url = record.get('URL')\n","    domain = record.get('domain')\n","    topic = record.get('topic')\n","    pub_date = record.get('publication_date')\n","    odia_sentence = record.get(SOURCE_SENTENCE_FIELD)\n","    german_sentence = record.get(TARGET_SENTENCE_FIELD)\n","\n","    # Skip if either sentence is missing\n","    if not isinstance(odia_sentence, str) or not isinstance(german_sentence, str):\n","      continue\n","\n","    # --- Create Instance 1: Odia -> German ---\n","    ori_to_deu_instance = {\n","        \"id\": new_id_counter,\n","        \"original_id\": original_id,\n","        \"URL\": url,\n","        \"domain\": domain,\n","        \"topic\": topic,\n","        \"publication_date\": pub_date,\n","        \"input_text\": PREFIX_ORI_TO_DEU + odia_sentence,\n","        \"target_text\": german_sentence\n","    }\n","    bidirectional_data.append(ori_to_deu_instance)\n","    new_id_counter += 1\n","\n","    # --- Create Instance 2: German -> Odia ---\n","    deu_to_ori_instance = {\n","        \"id\": new_id_counter,\n","        \"original_id\": original_id,\n","        \"URL\": url,\n","        \"domain\": domain,\n","        \"topic\": topic,\n","        \"publication_date\": pub_date,\n","        \"input_text\": PREFIX_DEU_TO_ORI + german_sentence,\n","        \"target_text\": odia_sentence\n","    }\n","    bidirectional_data.append(deu_to_ori_instance)\n","    new_id_counter += 1\n","\n","  print(\"Shuffling the new dataset...\")\n","  random.shuffle(bidirectional_data)\n","\n","  print(f\"Saving {len(bidirectional_data)} rich instances to '{OUTPUT_FILE}'...\")\n","  with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n","    for instance in bidirectional_data:\n","      f_out.write(json.dumps(instance, ensure_ascii=False) + '\\n')\n","\n","  print(\"\\n✅ Rich bidirectional dataset created successfully!\")"],"metadata":{"id":"cVcuhqlnZH2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","  create_rich_bidirectional_dataset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4e-9dQJCZJEB","executionInfo":{"status":"ok","timestamp":1750764827516,"user_tz":-120,"elapsed":672,"user":{"displayName":"Abhinandan Samal","userId":"05606082246540318546"}},"outputId":"fe6a770d-5d62-4d93-8049-5f0e6550e809"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading original structured corpus from '/content/drive/MyDrive/Thesis/data/transformed/authentic_corpus_final.jsonl'...\n","Creating bidirectional training instances with metadata...\n"]},{"output_type":"stream","name":"stderr","text":["Processing Records: 100%|██████████| 3676/3676 [00:00<00:00, 280832.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Shuffling the new dataset...\n","Saving 7352 rich instances to '/content/drive/MyDrive/Thesis/data/transformed/bidirectional_corpus_final.jsonl'...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Rich bidirectional dataset created successfully!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1c5WLwgZZPmY"},"execution_count":null,"outputs":[]}]}